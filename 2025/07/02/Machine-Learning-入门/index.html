

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/wallhaven-j5kjgy_1920x1080.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Jie">
  <meta name="keywords" content="">
  
    <meta name="description" content="重要概念Model FittingModel fitting 指, 通过调整参数, 让模型能够从训练数据中学习规律 (识别数据中的模式, 把 model fit 到数据上). 避免下面两种情况:  过拟合, 模型在训练集上表现极好, 在测试集上差 (可以理解为其记住了训练数据, 而不是学习到了规律) 欠拟合, 模型无法捕捉到数据的基本模式 (可能是模型太简单, 也可能是训练不足)  Predict">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine-Learning-入门">
<meta property="og:url" content="http://example.com/2025/07/02/Machine-Learning-%E5%85%A5%E9%97%A8/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="重要概念Model FittingModel fitting 指, 通过调整参数, 让模型能够从训练数据中学习规律 (识别数据中的模式, 把 model fit 到数据上). 避免下面两种情况:  过拟合, 模型在训练集上表现极好, 在测试集上差 (可以理解为其记住了训练数据, 而不是学习到了规律) 欠拟合, 模型无法捕捉到数据的基本模式 (可能是模型太简单, 也可能是训练不足)  Predict">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/machine-learning-k-fold-cross-validation.png">
<meta property="og:image" content="http://example.com/img/machine-learning-categorical-variable-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-binary-value-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-one-hot-encoding-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-basic-structure.png">
<meta property="og:image" content="http://example.com/img/ML-Ensemble-of-decision-trees.png">
<meta property="og:image" content="http://example.com/img/ml-decision-tree-example-structure.png">
<meta property="og:image" content="http://example.com/img/ml-entropy-equation-log2.png">
<meta property="og:image" content="http://example.com/img/ml-decision-tree-handle-continous-values.png">
<meta property="og:image" content="http://example.com/img/machine-learning-gradient-boosting-model.png">
<meta property="og:image" content="http://example.com/img/machine-learning-gradient-boosted-decision-tree.png">
<meta property="og:image" content="http://example.com/img/machine-learning-time-dummy-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-lag-feature-example.png">
<meta property="article:published_time" content="2025-07-02T08:55:01.000Z">
<meta property="article:modified_time" content="2025-08-31T01:53:24.813Z">
<meta property="article:author" content="Jie">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/machine-learning-k-fold-cross-validation.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Machine-Learning-入门 - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  

  

  



  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Jie</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/SteinsGate_all.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Machine-Learning-入门"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-07-02 16:55" pubdate>
          2025年7月2日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          22k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          180 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Machine-Learning-入门</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="重要概念"><a href="#重要概念" class="headerlink" title="重要概念"></a>重要概念</h1><h2 id="Model-Fitting"><a href="#Model-Fitting" class="headerlink" title="Model Fitting"></a>Model Fitting</h2><p>Model fitting 指, 通过调整参数, 让模型能够从训练数据中学习规律 (识别数据中的模式, 把 model fit 到数据上). 避免下面两种情况:</p>
<ul>
<li>过拟合, 模型在训练集上表现极好, 在测试集上差 (可以理解为其记住了训练数据, 而不是学习到了规律)</li>
<li>欠拟合, 模型无法捕捉到数据的基本模式 (可能是模型太简单, 也可能是训练不足)</li>
</ul>
<h2 id="Predicting"><a href="#Predicting" class="headerlink" title="Predicting"></a>Predicting</h2><p>Predicting 指, 用训练好的模型处理新的输入得到输出. 如果是分类任务, 则输出类别标签. 回归人物则输出连续值.</p>
<h2 id="Cross-validation"><a href="#Cross-validation" class="headerlink" title="Cross-validation"></a>Cross-validation</h2><p>Cross-validation 指, 通过将数据集划分为多个子集, 轮流作为训练集和验证集, 来评估模型的泛化能力.</p>
<p>Cross-validation 有多种实现方法, 比如 k-fold, 将数据打乱后 (shuffle) 分为 k 部分:</p>
<p><img src="/../img/machine-learning-k-fold-cross-validation.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li><code>StratifiedKFold</code> 能保证每一个 fold 中 positive 和 negative 的数据比例一致</li>
</ul>
<p>其他方法如 <code>Hold out</code>, <code>Leave one out</code> (每次留一个数据作为 validation set, 其他数据作为 train set) 等.</p>
<h2 id="Categorical-variable"><a href="#Categorical-variable" class="headerlink" title="Categorical variable"></a>Categorical variable</h2><p>Categorical variable (类别变量) 指变量的取值位于一个特定的集合中, 分为两类:</p>
<ul>
<li>nominal, 有两种及以上取值, 且没有顺序, 比如 <code>gender</code></li>
<li>ordinal, 取值间可以排序, 比如 <code>low</code>, <code>medium</code>, <code>high</code></li>
</ul>
<p>在 Machine learning 中, 将这些 features 转换为模型可识别的 integer, 可以用简单的映射 (ordinal encoding, 可以保证顺序):</p>
<p><img src="/../img/machine-learning-categorical-variable-example.png" srcset="/img/loading.gif" lazyload></p>
<p>有些模型需要二值化 (仅有 <code>0</code> 和 <code>1</code> 两种取值)的数据:</p>
<p><img src="/../img/machine-learning-binary-value-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>8 个取值只需要 3 位二进制, 16 个取值用 4 位二进制</li>
</ul>
<p>另一种方法为 one-hot encoding, 为每一个值建立一个 feeature (取值 <code>0</code> 或 <code>1</code>), 但要注意这种编码方式没有顺序:</p>
<p><img src="/../img/machine-learning-one-hot-encoding-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>在 <code>pandas</code> 中可以用 <code>get_dummies()</code> 实现, 由于其产生的是 sparse data (稀疏数据), 一般不用数组表示, 而是采用 sparse representation 如 <code>(0, 6)</code>, 表示第一行第六列为 <code>1</code></li>
<li>对于 cardinality 较多的 dataset 最好不用 one-hot encoding, 其会导致 dataset 过大</li>
</ul>
<p>Binarization 相比于 one-hot encoding 会产生更多的 <code>1</code>, 占用更大的空间.</p>
<h2 id="基本构成"><a href="#基本构成" class="headerlink" title="基本构成"></a>基本构成</h2><p>一个简单的 Machine learning 问题需要提供两部分内容:</p>
<ul>
<li><code>X</code>, Samples (包含 Features)</li>
<li><code>y</code>, Target</li>
</ul>
<p><img src="/../img/machine-learning-basic-structure.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>如果 target value 只有两种取值, 称 binary classification problem</li>
<li>如果 target value 有超过两种取值, 称 multi-class classification problem</li>
<li>若一个样本可以同时对应多个 target value, 则称 multi-label classification</li>
<li>若 target value 是浮点数, 则为 regression 问题, 也分 single column regression 和 multi-column regression</li>
</ul>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>Loss function, 损失函数, 用于量化模型预测与真实值之间的差异, 衡量模型预测的不准确程度, 数学表示为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>L(y,f(x)): R^n \times R^n \rightarrow R^+<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$y$ 是真实值</li>
<li>$f(x)$ 是模型预测值</li>
<li>$L$ 计算两者的差异</li>
</ul>
<p>常见的回归任务的损失函数有:</p>
<ul>
<li>均方误差, Mean Square Error:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\frac{1}{n} \sum_{i&#x3D;1}^n (y_i - f(x_i))^2<br>\end{aligned}<br>}<br>$$</li>
<li>平均绝对误差, Mean Absolute Error<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\frac{1}{n} \sum_{i&#x3D;1}^n || y_i - f(x_i) ||<br>\end{aligned}<br>}<br>$$</li>
</ul>
<p>常见的分类任务的损失函数有:</p>
<ul>
<li>交叉熵损失:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\sum_{i&#x3D;1}^n y_i log(f(x_i))<br>\end{aligned}<br>}<br>$$</li>
</ul>
<h2 id="arg-min-和-arg-max"><a href="#arg-min-和-arg-max" class="headerlink" title="arg min 和 arg max"></a>arg min 和 arg max</h2><p><code>arg</code> 指 argument, 在数学优化中表示: 是函数取值最xx时的变量取值. 比如:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\gamma* &#x3D; arg min_{\gamma} f(\gamma)<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>表示找到使函数 $f(\gamma)$ 取得最小值的 \gamma 值, 即 $\gamma^*$</li>
</ul>
<h2 id="Residual"><a href="#Residual" class="headerlink" title="Residual"></a>Residual</h2><p>Residual, 残差, 表示观测值与模型预测值之间的差异:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>Residual &#x3D; y_i - f(x_i)<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$y_i$ 表示第 $i$ 个观测的真实值</li>
<li>$f(x_i)$ 是模型对第 $i$ 个样本的预测值</li>
</ul>
<p>“残差” 一词中:</p>
<ul>
<li>“残” 表示剩余, 残余</li>
<li>“差” 表示差异</li>
<li>核心表达: 模型未能解释的数据变异部分</li>
</ul>
<h2 id="Derivative"><a href="#Derivative" class="headerlink" title="Derivative"></a>Derivative</h2><p>Derivative, 导数, 表示函数曲线在某点的切线斜率, 也就是函数变化的方向:</p>
<ul>
<li>正导数 $\rightarrow$ 函数递增</li>
<li>负导数 $\rightarrow$ 函数递减</li>
<li>零导数 $\rightarrow$ 可能为极值点或驻点</li>
</ul>
<p>对于函数 $f(x)$, 在点 $x&#x3D;a$ 处的导数定义为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>f’(a) &#x3D; \underset{h \rightarrow 0}{lim \frac{f(a+h) - f(a)}{h}}<br>\end{aligned}<br>}<br>$$</p>
<h2 id="Partial-Derivative"><a href="#Partial-Derivative" class="headerlink" title="Partial Derivative"></a>Partial Derivative</h2><p>Partial Derivative, 偏导数, 描述函数在某一变量方向上的变化率, 同时保持其他变量不变.</p>
<p>对于多元函数 $f(x_1,x_2,…,x_n)$, 在点 $(a_1, a_2,…, a_n)$ 处关于 $x_1$ 的偏导数定义为:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\frac{\partial f}{\partial x_i} &#x3D; \underset{h \rightarrow 0}{lim} \frac{f(a_1,…,a_i+h,…,a_n) - f(a_1,…,a_i,…,a_n)}{h}<br>\end{aligned}<br>}<br>$$</p>
<p>计算 partial derivative 时, 将其他变量视为常数, 对目标变量求导.</p>
<h2 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h2><p>“Gradient” 源自拉丁语 “gradus” (意味 “步级”, “程度”) 和 “-ient” 后缀 (表示 “进行中”), 组合含义为 “逐步变化的过程”.</p>
<p>对于 n 元函数 $f(x_1,x_2,…,x_n$, 其梯度表示为:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\nabla f &#x3D; (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2},…,\frac{\partial f}{\partial x_n})^T<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$\nabla$ 是梯度算子</li>
<li>$\frac{\partial f}{\partial x_i}$ 是 $f$ 对第 $i$ 个变量的偏导数</li>
<li>$T$ 转置将默认的行向量转为列向量 </li>
<li>梯度是一个向量, 其维度与自变量相同</li>
<li>这里计算的结果, 其实是每个方向偏导数构成的一个向量, 如果此时要取单个方向, 乘以对应的单位方向向量即可</li>
</ul>
<p>梯度是 “指向函数值最大增长率方向”, 证明如下:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>D_u f &#x3D; \nabla f \cdot u &#x3D; || \nabla f || cos \theta<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$D_u f$ 表示 $f$ 在方向 $u$ 上的方向导数</li>
<li>$\theta$ 是梯度向量 $\nabla f$ 与方向 $u$ 的夹角</li>
<li>要使得 $D_u f$ 最大, 需要 $cos \theta &#x3D; 1$, $\theta &#x3D; 0$, 此时 $u$ 与 $\nabla f$ 同向</li>
</ul>
<h2 id="Data-Leakage"><a href="#Data-Leakage" class="headerlink" title="Data Leakage"></a>Data Leakage</h2><p>Data Leakage 指训练数据中包含 target 的信息, 导致 model 在 trainning set 上表现.</p>
<p>比如: 房价预测模型, 用整个数据集的平均房价对特征进行标准化, 其包含了测试集的信息, 因此会在测试集表现良好, 正确做法是只使用训练集的统计量, 不包含测试集.</p>
<h2 id="Mean"><a href="#Mean" class="headerlink" title="Mean"></a>Mean</h2><p>Mean, 算术均值, 对于包含 n 个数据点的数据集 $X &#x3D; {x_1, x_2, …, x_n}$, 其算术均值的计算为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\mu &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^n x_i<br>\end{aligned}<br>}<br>$$</p>
<p>如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">data = [<span class="hljs-number">12</span>, <span class="hljs-number">15</span>, <span class="hljs-number">18</span>, <span class="hljs-number">21</span>, <span class="hljs-number">24</span>]<br>mean = <span class="hljs-built_in">sum</span>(data) / <span class="hljs-built_in">len</span>(data)<br></code></pre></td></tr></table></figure>

<h2 id="Standard-deviation"><a href="#Standard-deviation" class="headerlink" title="Standard deviation"></a>Standard deviation</h2><p>Standard deviation 用于衡量数据的离散程度, 数学表达为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\sigma &#x3D; \sqrt{\frac{1}{n} \sum_{i&#x3D;1}^n (x_i - \mu)^2}<br>\end{aligned}<br>}<br>$$</p>
<h2 id="Standardization"><a href="#Standardization" class="headerlink" title="Standardization"></a>Standardization</h2><p>Standardization, 标准化, 用于将所有特征转换到相似的范围内, 因为不同特征的量纲和范围差异大.</p>
<p>标准化 (Z-score 标准化) 的数学表达式为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>z &#x3D; \frac{x - \mu}{\sigma}<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$x$ 是原始特征值</li>
<li>$\mu$ 是特征的均值</li>
<li>$\sigma$ 是特征的标准差</li>
<li>$z$ 是标准化之后的值</li>
</ul>
<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><p>Normalization, 归一化, 将数据按比例缩放到特定区间, 最常见的是压缩到 <code>[0,1]</code> 或 <code>[-1,1]</code> 区间.</p>
<p>可以消除量纲的影响, 防止数值溢出.</p>
<p>Min-Max 归一化算法的数学表达为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>x’ &#x3D; \frac{x - min(X)}{max(X) - min(X)}<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>结果范围: <code>[0,1]</code></li>
</ul>
<p>MaxAbs 归一化算法的数学表达:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>x’ &#x3D; \frac{x}{max(|X|)}<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>结果范围: <code>[-1,1]</code></li>
</ul>
<h2 id="Polynomial-Features"><a href="#Polynomial-Features" class="headerlink" title="Polynomial Features"></a>Polynomial Features</h2><p>Polynomial Features, 多项式特征, 指通过创建原始特征的组合来扩展特征空间, 帮助模型捕获更复杂的数据关系.</p>
<p>比如将线性特征转换为非线性特征:</p>
<ul>
<li>原始特征: $x_1$, $x_2$</li>
<li>二次项多项式: $x_1^2$, $x_1x_2$, x_2^2</li>
<li>等等</li>
</ul>
<p>比如创建 interactive 特征:</p>
<ul>
<li>$x_1x_2$, $x_1x_3$, $x_2x_3$ 等交叉项</li>
</ul>
<h2 id="Binning-x2F-Bucket"><a href="#Binning-x2F-Bucket" class="headerlink" title="Binning&#x2F;Bucket"></a>Binning&#x2F;Bucket</h2><p>Binning, 分箱, 是一种将连续的数值特征离散化为若干个区间的技术 (有时也用 bucket 做术语).</p>
<p>可以将连续变量转化为有序类别变量.</p>
<h2 id="Target-Encoding"><a href="#Target-Encoding" class="headerlink" title="Target Encoding"></a>Target Encoding</h2><p>Target Encoding, 字面意思, 对 target 的数据进行预处理. 一般来说, 是用 target 的统计量来代替类别值, 对于分类问题, 用目标类别概率, 对回归问题, 用目标均值, 比如:</p>
<p>原始数据:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs">┌───────────┬────────┐<br>│ 城市      │ 房价   │<br>├───────────┼────────┤<br>│ 北京      │ 8.5    │<br>│ 上海      │ 9.2    │<br>│ 北京      │ 8.8    │<br>│ 广州      │ 6.5    │<br>│ 上海      │ 9.0    │<br>└───────────┴────────┘<br></code></pre></td></tr></table></figure>

<p>Target Encoding 之后:</p>
<figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs haxe">┌───────────┬──────────────┐<br>│ 城市      │ 城市<span class="hljs-literal">_</span>平均房价 │<br>├───────────┼──────────────┤<br>│ 北京      │ <span class="hljs-number">8.65</span>         │<br>│ 上海      │ <span class="hljs-number">9.10</span>         │<br>│ 广州      │ <span class="hljs-number">6.50</span>         │<br>└───────────┴──────────────┘<br></code></pre></td></tr></table></figure>

<p>可以使用 k-fold 的思想来计算.</p>
<h2 id="Model-blending"><a href="#Model-blending" class="headerlink" title="Model blending"></a>Model blending</h2><p>Model blending, 模型融合, 指用不同的模型得到 predictions, 然后用这些 predictions 构成新的 target.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">df = pd.read_csv(<span class="hljs-string">&quot;train_data.csv&quot;</span>)<br>df_test = pd.read_csv(<span class="hljs-string">&quot;test_data.csv&quot;</span>)<br><br>df1 = pd.read_csv(<span class="hljs-string">&quot;train_pred_1.csv&quot;</span>)<br>df2 = pd.read_csv(<span class="hljs-string">&quot;train_pred_2.csv&quot;</span>)<br>df3 = pd.read_csv(<span class="hljs-string">&quot;train_pred_3.csv&quot;</span>)<br><br>df_test1 = pd.read_csv(<span class="hljs-string">&quot;test_pred_1.csv&quot;</span>)<br>df_test2 = pd.read_csv(<span class="hljs-string">&quot;test_pred_2.csv&quot;</span>)<br>df_test3 = pd.read_csv(<span class="hljs-string">&quot;test_pred_3.csv&quot;</span>)<br><br>df = df.merge(df1, on=<span class="hljs-string">&quot;id&quot;</span>, how=<span class="hljs-string">&quot;left&quot;</span>)<br>df = df.merge(df2, on=<span class="hljs-string">&quot;id&quot;</span>, how=<span class="hljs-string">&quot;left&quot;</span>)<br>df = df.merge(df3, on=<span class="hljs-string">&quot;id&quot;</span>, how=<span class="hljs-string">&quot;left&quot;</span>)<br><br>df_test = df_test.merge(df_test1, on=<span class="hljs-string">&quot;id&quot;</span>, how=<span class="hljs-string">&quot;left&quot;</span>)<br>df_test = df_test.merge(df_test2, on=<span class="hljs-string">&quot;id&quot;</span>, how=<span class="hljs-string">&quot;left&quot;</span>)<br>df_test = df_test.merge(df_test3, on=<span class="hljs-string">&quot;id&quot;</span>, how=<span class="hljs-string">&quot;left&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>注意 <code>id</code> 的匹配以及不同 model 的权重设置.</p>
<p>相当于是用一些 model 来构建一个新的 target, 然后用另一个 model 基于这个 target 进行训练.</p>
<h2 id="Model-Stacking"><a href="#Model-Stacking" class="headerlink" title="Model Stacking"></a>Model Stacking</h2><p>Model Stacking, 模型堆叠, 指用原始数据训练一些基模型, 得到多个预测值, 称为 meta feature, 然后用一个模型学习 meta feature.</p>
<figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs gams">原始特征<br>├─ Base <span class="hljs-keyword">Model</span> <span class="hljs-number">1</span> → OOF Pred <span class="hljs-number">1</span><br>├─ Base <span class="hljs-keyword">Model</span> <span class="hljs-number">2</span> → OOF Pred <span class="hljs-number">2</span><br>└─ Base <span class="hljs-keyword">Model</span> <span class="hljs-number">3</span> → OOF Pred <span class="hljs-number">3</span><br>↓<br>[OOF Pred <span class="hljs-number">1</span>, OOF Pred <span class="hljs-number">2</span>, OOF Pred <span class="hljs-number">3</span>] → Meta <span class="hljs-keyword">Model</span><br>↓<br>最终预测<br></code></pre></td></tr></table></figure>

<h2 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h2><p>Time Series 指一组随时间变化的观测值. 时间的截取一般有固定的频率, 比如 daily or monthly.</p>
<h2 id="Trend"><a href="#Trend" class="headerlink" title="Trend"></a>Trend</h2><p>Time series 中缓慢变化或保持不变的部分称为 Trend.</p>
<h2 id="QKV"><a href="#QKV" class="headerlink" title="QKV"></a>QKV</h2><p>在 Transformer 架构中, QKV vector 是 self-attention 的核心组件, 用于动态计算关系权重.</p>
<ul>
<li>Q, Query, 表示当前需要获取信息的向量, 决定关注哪些位置的信息</li>
<li>K, Key, 表示其他位置的标识向量, 用于 Query 中匹配</li>
<li>V, Value, 包含实际信息的向量</li>
</ul>
<p>即, 用 Q 查询 K 得到 V.</p>
<p>输入序列的每个词向量通过线性变换生成对应的 Q, K, V:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>Q &#x3D; XW_Q,\ K &#x3D; XW_K,\ V &#x3D; XW_V<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li><code>X</code> 是输入词向量</li>
<li>$W_Q$, $W_K$, $W_V$ 是可学习的权重矩阵, 是可调整的参数</li>
</ul>
<p>计算注意力权重:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>Attention(Q,K,V) &#x3D; softmax(\frac{QK^T}{\sqrt{d_k}}) V<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>输出就是 <code>Value</code> 向量的加权和, 权重由 Query 和 Key 的相似度决定, 输出的序列长度由 <code>Query</code> 决定, 输出的特征维度由 Value 决定</li>
</ul>
<h2 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h2><p>Pooling, 池化, 是一种对局部区域的聚合操作 (如取最大值, 平均值等).</p>
<p>Pool 英语中意为 “水池”, 可以理解为将多个小水流汇聚到一个池子中的过程.</p>
<p>在 Pooling 操作中, 每个局部窗口 (比如 <code>2x2</code> 像素块) 的数值被 “汇聚” 成一个代表值 (如最大值或平均值), 就像多个小水流汇入一个池子.</p>
<h2 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h2><p>MNIST, Modified National Institute of Standards and Technology, 是机器学习领域最经典的手写数字识别数据集, 广泛用于算法验证, 教学和基准测试.</p>
<p>该数据集包含 <code>0-9</code> 的手写数字灰度图片:</p>
<ul>
<li>60,000 张训练集图片</li>
<li>10,000 张测试集图片</li>
<li>尺寸为 <code>28x28</code> 像素</li>
<li>颜色为灰度, 单通道 <code>0-255</code> 像素值</li>
</ul>
<h2 id="End-to-End"><a href="#End-to-End" class="headerlink" title="End-to-End"></a>End-to-End</h2><p>End-to-End 指, 输入原始的图片, 文字, 不做任何预处理, 然后直接通过机器学习得到结果.</p>
<h2 id="Vanishing-Gradients-和-Exploding-Gradients"><a href="#Vanishing-Gradients-和-Exploding-Gradients" class="headerlink" title="Vanishing Gradients 和 Exploding Gradients"></a>Vanishing Gradients 和 Exploding Gradients</h2><p>Vanishing Gradients (梯度消失) 和 Exploding Gradients (梯度爆炸) 都源自深度神经网络中的反向传播算法.</p>
<h3 id="Vanishing-Gradients"><a href="#Vanishing-Gradients" class="headerlink" title="Vanishing Gradients"></a>Vanishing Gradients</h3><p>Vanishing Gradients 指在反向传播过程中, 梯度值变得非常非常小, 无限接近于 0. 因为链式法则中的连续乘法, 假设每一层传递梯度时都乘以一个小于 1 的因子 (例如 sigmoid 函数的导数最大值是 <code>0.25</code>), 经过很多层之后, 这些小于 1 的数连续相乘, 放结果指数级趋近 0:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\frac{\partial Loss}{\partial W_{early}} \approx \frac{\partial Loss}{\partial W_{late}} \cdot (小导数) \cdot (小导数) \cdot (小导数) … \rightarrow 0<br>\end{aligned}<br>}<br>$$</p>
<p>这会导致网络深层的权重能有效更新 (因为梯度还正常), 但浅层的权重几乎得不到更新 (梯度趋于 0). 此时网络无法进一步优化.</p>
<p>想象一个 10 层的网络. 在更新第一层的参数时, 梯度需要穿过后面 9 层才能传回来. 如果每一层平均使梯度缩小一半 (乘以0.5), 那么传到第一层的梯度就是最初的梯度乘以 $0.5^9&#x3D;1&#x2F;512$. 这个值已经非常小了, 如果网络更深, 梯度会小到计算机无法表示 (下溢).</p>
<h3 id="Exploding-Gradients"><a href="#Exploding-Gradients" class="headerlink" title="Exploding Gradients"></a>Exploding Gradients</h3><p>Exploding Gradients 指在反向传播过程中, 梯度值变得非常大, 最终变成 <code>NaN</code>.</p>
<p>同样是由于链式乘法, 如果每一层传递梯度时都乘以一个大于 1 的因子, 经过很多层之后, 导致结果指数级增长:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\frac{\partial Loss}{\partial W_{early}} \approx \frac{\partial Loss}{\partial W_{late}} \cdot (大导数) \cdot (大导数) \cdot (大导数) … \rightarrow 0<br>\end{aligned}<br>}<br>$$</p>
<p>梯度值大到超出计算机浮点数的表示范围, 变成 <code>NaN</code>. 一旦出现 <code>NaN</code>, 后续所有计算都会失效.</p>
<h2 id="Dot-product"><a href="#Dot-product" class="headerlink" title="Dot product"></a>Dot product</h2><p>向量内积 (点积, dot product), 可以用下面的写法:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>&lt;a, b&gt; &#x3D; a \cdot b<br>\end{aligned}<br>}<br>$$</p>
<h2 id="Likelihood"><a href="#Likelihood" class="headerlink" title="Likelihood"></a>Likelihood</h2><p>Likelihood, 似然, 表示给定某个参数条件下, 观察到当前数据的概率. (已知数据, 尝试多组参数值)</p>
<p>需要区分 probability 问题 (已知参数 $\theta$, 求数据 $D$ 的概率) 和 likelihood 问题 (已知数据 $D$, 评估不同参数 $\theta$ 的可能性), 比如:</p>
<ol>
<li>一个 probability 问题: 如果硬币公平 $\theta &#x3D; 0.5$, 那么得到 $7$ 正 $3$ 反的概率是多少:</li>
</ol>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>P(D|\theta&#x3D;0.5) &#x3D; C(10,7) \times (0.5)^7 \times (0.5)^3 \approx 0.117<br>\end{aligned}<br>}<br>$$</p>
<h1 id="Random-forest-model"><a href="#Random-forest-model" class="headerlink" title="Random forest model"></a>Random forest model</h1><p>Random forest model, 随机森林模型, 是一种 “集成学习 (Ensemble learning)” (将多个小模型组合成一个大模型) 的机器学习算法, 由多个决策树 (Decision trees, 也就是这里的小模型) 组成. 利用决策树的结果进行 “少数服从多数” 的投票来提高准确性. 适用于分类和回归.</p>
<ul>
<li>随机, 指随机从数据集中采样, 随机选择特征, 以分别训练模型中的每棵决策树 (每次都随机)</li>
<li>森林, 指多个决策树</li>
</ul>
<p><img src="/../img/ML-Ensemble-of-decision-trees.png" srcset="/img/loading.gif" lazyload></p>
<p>当完成训练后, 将待测样本输入所有决策树, 并投票得出最后结果.</p>
<p>随机森林的优势在于可解释性很强, 能进行特征选择 (知道特征的重要性).</p>
<h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><p>集成学习, Ensemble learning, 指内部由多个弱监督模型 (weak learner) 组成, 而每个若监督模型只在一个方面表现好.</p>
<p>有几个常见类型:</p>
<ul>
<li>Bagging: 指并行训练多个分类器并取平均 $f(x) &#x3D; \frac{1}{M} \sum_{m&#x3D;1}^M f_m (x)$</li>
<li>Boosting: 把一个弱学习器慢慢加强, 通过加权来训练</li>
<li>Stacking: 聚合多个分类或回归模型</li>
</ul>
<p>随机森林是一种 Bagging 算法.</p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>决策树, Decision tree, 是一种树形结构的监督学习算法, 通过一系列规则对数据进行分类或回归 (数据都有特征, 通过特征来区分).</p>
<ul>
<li>每个内部节点表示对一个特征的决策规则, 如 <code>age &gt; 30</code></li>
<li>根节点是第一个决策规则</li>
<li>分支是节点的输出, 如 <code>true</code>, <code>false</code></li>
<li>叶子节点是分类的结果 (或者回归的结果)</li>
</ul>
<p>当我们有一组特征信息, 但不知道其类别, 将其输入决策树, 经过一系列决策条件, 就能得到类别.</p>
<p>每经过一个节点, 都将原有数据分为两部分.</p>
<p><img src="/../img/ml-decision-tree-example-structure.png" srcset="/img/loading.gif" lazyload></p>
<p>选择合适的决策规则 (也就是找到最佳特征) 可以提升分类效率.</p>
<p>越有效的特征越先进行决策划分.</p>
<h3 id="决策树的训练和测试"><a href="#决策树的训练和测试" class="headerlink" title="决策树的训练和测试"></a>决策树的训练和测试</h3><p>训练指, 从给定的训练集构建决策树. 如何选择特征并安排节点次序, 如何用特征构建规则 (如何切分特征).</p>
<p>测试指, 向决策树输入测试数据, 观察结果.</p>
<p>可以通过 “熵” 来判断一个特征的有效程度 (用于于其他类别区分的重要程度).</p>
<p>熵, 定义为随机变量的不确定性的度量, 也就是物体内部的混乱程度. 在当前上下文中, 如果一组数据中, 类别越少, 则熵值越低, 类别越多, 熵值越高. 用来计算熵值的公式为:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>H(S) &#x3D; -\sum_{i&#x3D;1}^c p_i log_2 p_i<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li><p>$H(S)$ 中 <code>S</code> 指数据集 (Set)</p>
</li>
<li><p>$p_i$ 指第 $i$ 类样本所占比例 (概率, probability)</p>
</li>
<li><p>$c$ 指类别的总数</p>
</li>
<li><p>这里 $log_2$ 的作用是, 概率越大, 负值越小, 算出来的熵越小<br><img src="/../img/ml-entropy-equation-log2.png" srcset="/img/loading.gif" lazyload></p>
</li>
</ul>
<p>例如:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">A = <span class="hljs-selector-attr">[1,1,1,1,1,1,1,1,2,2]</span><br>B = <span class="hljs-selector-attr">[1,2,3,4,5,6,7,8,9,1]</span><br><span class="hljs-function"><span class="hljs-title">H</span><span class="hljs-params">(A)</span></span>: <span class="hljs-number">0.7219280948873623</span><br><span class="hljs-function"><span class="hljs-title">H</span><span class="hljs-params">(B)</span></span>: <span class="hljs-number">3.121928094887362</span><br></code></pre></td></tr></table></figure>
<ul>
<li>集合 <code>A</code> 的熵值低很多</li>
</ul>
<p>对于一个特征而言, 若集合通过后熵越低, 表示剩下的类别越越少, 用于筛选效果越好.</p>
<h4 id="ID3-算法"><a href="#ID3-算法" class="headerlink" title="ID3 算法"></a>ID3 算法</h4><p>一个核心概念 “信息增益”, 表示特征 <code>X</code> 使得类 <code>Y</code> 的不确定性减少的程度. (两个熵的差值, 越大越好)</p>
<p>为了找到合适的特征, 首先要计算原始数据的熵值, 然后分别用各个特征作为根节点, 对数据做处理后, 再次计算熵值 (注意加权相加), 熵下降越多, 表示越有效, 越合适.</p>
<p>该算法的问题在于, 不适合处理非常稀疏的特征, 比如 <code>ID</code> 值, 10 个 ID 经过该节点后分为 10 部分, 每个部分只包含 1 个值, 此时计算出的熵为 0, 但其显然不能用于决策.</p>
<h4 id="C4-5-算法"><a href="#C4-5-算法" class="headerlink" title="C4.5 算法"></a>C4.5 算法</h4><p>C4.5 算法是 ID3 算法的改进算法, 利用信息增益率来选择特征.</p>
<p>信息增益率通过引入分裂信息 (split information) 来惩罚多值特征 (比如 <code>ID</code>), 公式为:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>Gain\ Ratio &#x3D; \frac{Information\ Gain}{Split\ Information} \<br>Information\ Gain(S,A) &#x3D; H(S) - H(S|A) \<br>Split\ Information(A) &#x3D; -\sum_{i&#x3D;1}^v \frac{|S_i|}{|S|} log_2 \frac{|S_i|}{|S|}<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li><code>v</code> 是分裂数, (如 “年龄” 分青年, 中年, 老年, 则 <code>v=3</code>)</li>
<li>$|S_i|$ 是第 $i$ 个子集的样本数</li>
</ul>
<p>增益率越大越先考虑某特征.</p>
<h4 id="CART-算法"><a href="#CART-算法" class="headerlink" title="CART 算法"></a>CART 算法</h4><p>采用 GINI 系数作为衡量标准, 计算方法为:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>Gini(p) &#x3D; \sum_{i&#x3D;1}^c p_i (1-p_i) &#x3D; 1 - \sum_{i&#x3D;1}^c p_i^2<br>\end{aligned}<br>}<br>$$</p>
<p>同样是越小越好.</p>
<h4 id="连续值的处理"><a href="#连续值的处理" class="headerlink" title="连续值的处理"></a>连续值的处理</h4><p>先对数据进行排序, 然后不断进行 “二分” 并计算信息增益 (或者 Gini 系数等) 来判断分界点.</p>
<p><img src="/../img/ml-decision-tree-handle-continous-values.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h4><p>对于回归问题, 特征用方差来选择.</p>
<h4 id="剪枝策略"><a href="#剪枝策略" class="headerlink" title="剪枝策略"></a>剪枝策略</h4><p>防止过拟合 (在训练集上表现好, 测试集表现差). 毕竟只要你无限拆分, 就能完全分开一个数据集 (每个类别一个元素), 但此时不具有任何通用性了.</p>
<p>剪枝主要是: 限制深度, 叶子节点个数, 叶子节点样本数, 信息增益量等. 防止决策树无限增长.</p>
<p>剪枝策略有:</p>
<ul>
<li>预剪枝, 建立决策树时进行剪枝</li>
<li>后剪枝, 决策树建立完成后剪枝</li>
</ul>
<h3 id="构建决策树-示例"><a href="#构建决策树-示例" class="headerlink" title="构建决策树 (示例)"></a>构建决策树 (示例)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 计算熵</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">entropy</span>(<span class="hljs-params">labels</span>):<br>    _, counts = np.unique(labels, return_counts=<span class="hljs-literal">True</span>)<br>    probabilities = counts / <span class="hljs-built_in">len</span>(labels)<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(probabilities * np.log2(probabilities))<br><br><span class="hljs-comment"># 计算信息增益</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">information_gain</span>(<span class="hljs-params">data, feature, target</span>):<br>    total_entropy = entropy(data[target])<br>    values, counts = np.unique(data[feature], return_counts=<span class="hljs-literal">True</span>)<br>    weighted_entropy = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> v, c <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(values, counts):<br>        subset = data[data[feature] == v][target]<br>        weighted_entropy += (c / <span class="hljs-built_in">len</span>(data)) * entropy(subset)<br>    <span class="hljs-keyword">return</span> total_entropy - weighted_entropy<br><br><span class="hljs-comment"># 选择最佳分裂特征</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">choose_best_feature</span>(<span class="hljs-params">data, features, target</span>):<br>    gains = [information_gain(data, f, target) <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features]<br>    <span class="hljs-keyword">return</span> features[np.argmax(gains)]<br><br><span class="hljs-comment"># 递归构建决策树</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_tree</span>(<span class="hljs-params">data, features, target, tree=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-keyword">if</span> tree <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        tree = &#123;&#125;<br>    <br>    <span class="hljs-comment"># 终止条件1：所有样本属于同一类别</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(pd.unique(data[target])) == <span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> data[target].iloc[<span class="hljs-number">0</span>]<br>    <br>    <span class="hljs-comment"># 终止条件2：无特征可用</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(features) == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> data[target].mode()[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 返回多数类</span><br>    <br>    <span class="hljs-comment"># 选择最佳特征</span><br>    best_feature = choose_best_feature(data, features, target)<br>    tree[best_feature] = &#123;&#125;<br>    <br>    <span class="hljs-comment"># 递归分裂</span><br>    <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> pd.unique(data[best_feature]):<br>        subset = data[data[best_feature] == value]<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(subset) == <span class="hljs-number">0</span>:<br>            tree[best_feature][value] = data[target].mode()[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 处理空子集</span><br>        <span class="hljs-keyword">else</span>:<br>            remaining_features = [f <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features <span class="hljs-keyword">if</span> f != best_feature]<br>            tree[best_feature][value] = create_tree(subset, remaining_features, target)<br>    <br>    <span class="hljs-keyword">return</span> tree<br><br><br><span class="hljs-comment"># 数据集</span><br>dataset = pd.DataFrame(&#123;<br>    <span class="hljs-string">&quot;F1-AGE&quot;</span>: [<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>],<br>    <span class="hljs-string">&quot;F2-WORK&quot;</span>: [<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],<br>    <span class="hljs-string">&quot;F3-HOME&quot;</span>: [<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],<br>    <span class="hljs-string">&quot;F4-LOAN&quot;</span>: [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>],<br>    <span class="hljs-string">&quot;STATUS&quot;</span>: [<span class="hljs-string">&quot;no&quot;</span>,<span class="hljs-string">&quot;no&quot;</span>,<span class="hljs-string">&quot;yes&quot;</span>,<span class="hljs-string">&quot;yes&quot;</span>,<span class="hljs-string">&quot;no&quot;</span>,<span class="hljs-string">&quot;no&quot;</span>,<span class="hljs-string">&quot;no&quot;</span>,<span class="hljs-string">&quot;no&quot;</span>,<span class="hljs-string">&quot;yes&quot;</span>,<span class="hljs-string">&quot;yes&quot;</span>,<span class="hljs-string">&quot;yes&quot;</span>,<span class="hljs-string">&quot;yes&quot;</span>,<span class="hljs-string">&quot;yes&quot;</span>,<span class="hljs-string">&quot;yes&quot;</span>,<span class="hljs-string">&quot;no&quot;</span>]<br>&#125;)<br><br><span class="hljs-comment"># 特征列表</span><br>labels = [<span class="hljs-string">&quot;F1-AGE&quot;</span>, <span class="hljs-string">&quot;F2-WORK&quot;</span>, <span class="hljs-string">&quot;F3-HOME&quot;</span>, <span class="hljs-string">&quot;F4-LOAN&quot;</span>]<br><br><span class="hljs-comment"># 构建决策树</span><br>decision_tree = create_tree(dataset, labels, <span class="hljs-string">&quot;STATUS&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;决策树结构：&quot;</span>, decision_tree)<br></code></pre></td></tr></table></figure>

<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><ol>
<li>预测模型的超参数, 几棵树? 树的深度?</li>
<li>随机采样, 训练每个决策树</li>
<li>输入待测样本到各个树中, 再将每个树的结果整合:<ul>
<li>对于 Regression 问题, 通常用均值来整合</li>
<li>对于 Classification 问题, 通常求众数</li>
</ul>
</li>
</ol>
<h1 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h1><p>Gradient Boosting 是一种集成学习算法 (属于 Boosting 方法), 通过迭代添加弱学习器 (通常是决策树) 来纠正前一个模型的错误, 最终组合成一个强预测模型 (向前分步加法模型).</p>
<p><img src="/../img/machine-learning-gradient-boosting-model.png" srcset="/img/loading.gif" lazyload></p>
<p>其模型的训练是 “顺序进行”, 不同于随机森林的并行.</p>
<p>核心数学框架是利用梯度下降思想优化损失函数:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>L(\theta) &#x3D; \sum_{i&#x3D;1}^n L (y_i, F(x_i; \theta))<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$L$ 是可微的损失函数</li>
<li>$F$ 是集成模型</li>
<li>$\theta$ 是模型参数</li>
</ul>
<p>大致过程如下:</p>
<p><img src="/../img/machine-learning-gradient-boosted-decision-tree.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>注意 $f_2(x_i) \rightarrow y_i - \hat{y}_i^1$, 利用真实值与前一个模型的差值计算新模型</li>
<li>$\hat{y}_i^2 &#x3D; \hat{y}_i^1 + f_2(x_i)$ 通过上一个模型加上利用差值构建的模型来构建新模型</li>
</ul>
<h2 id="算法具体步骤"><a href="#算法具体步骤" class="headerlink" title="算法具体步骤"></a>算法具体步骤</h2><p>初始化:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>f_0(x) &#x3D; arg\ \underset{\gamma}{min} \sum_{i&#x3D;1}^N L(y_i, \gamma)<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>找到一个让损失最小的预测值 $\gamma$</li>
</ul>
<p>对于 $m &#x3D; 1$ 到 $M$, 进入循环, 计算残差 (pseudo-residuals):<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>r_{im} &#x3D; [\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}]<em>{f&#x3D;f</em>{m-1}}<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>计算损失函数在当前模型 $f_{m-1}$ 下的负梯度 (梯度的方向指示了如何调整预测以减少损失)</li>
</ul>
<p>然后拟合回归树:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>Fit\ tree\ to\ r_{im}\ giving\ terminal\ regions\ R_{jm},j&#x3D;1,…,J_m<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>用伪残差 $r_{im}$  作为新的目标变量来训练一棵回归树</li>
<li>$R_{jm}$ 表示树的第 $j$ 个叶节点对应的输入空间区域</li>
<li>$J_m$ 是当前树的叶节点数量</li>
</ul>
<p>计算叶节点值 (预测值):<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\gamma_{jm} &#x3D; arg\ \uderset{\gamma}{min} \sum_{x_i \in R_{jm}} L (y_i, f_{m-1}(x_i) + \gamma)<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>为每个叶节点 $R_{jm}$ 寻找最优的增量 $\gamma_{jm}$</li>
</ul>
<p>更新模型:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>f_m(x) &#x3D; f_{m-1}(x) + \sum_{j&#x3D;1}^{J_m} \gamma_{jm} I (x \in R_{jm})<br>\end{aligned}<br>}<br>$$</p>
<p>最终得到输出:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\hat{f}(x) &#x3D; f_M(x)<br>\end{aligned}<br>}<br>$$</p>
<h1 id="Feature-Engineering"><a href="#Feature-Engineering" class="headerlink" title="Feature Engineering"></a>Feature Engineering</h1><p>Feature Engineering, 特征工程, 是将原始数据转化为模型可理解的特征的过程. 好的特征可以提升模型性能并减少不必要的训练.</p>
<p>可以用原始数据构造新特征 (Feature Creation), 比如:</p>
<ul>
<li>组合特征: “单价&#x3D;总价&#x2F;数量”</li>
</ul>
<p>进行特征选择 (Feature Selection), 选择最有价值的特征, 如:</p>
<ul>
<li>过滤法: 基于统计指标 (如相关系数)</li>
</ul>
<p>特征变换 (Feature Transformation), 改变特征分布或表示形式, 如:</p>
<ul>
<li>标准化: Z-score 标准化</li>
<li>归一化: Min-Max 缩放</li>
<li>非线性变换: 对数变换</li>
</ul>
<h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><p>Linear Regression, 线性回归, 用于建模连续目标变量 $Y$ 与一个或多个特征变量 $X$ 之间的线性关系, 其核心思想是找到一条最佳拟合的直线 (注意是直线而非曲线), 使得预测值与真实值之间的误差最小.</p>
<p>对于单变量的线性回归, 可以建模为:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>Y &#x3D; \beta_0 + \beta_1 X + \epsilon<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$Y$ 是目标变量, 因变量</li>
<li>$X$ 是特征变量, 自变量</li>
<li>$\beta_0$ 是截距, 与 $y$ 轴的交点</li>
<li>$\beta_1$ 是斜率, $X$ 每变化 $1$ 单位, $Y$ 的变化量</li>
<li>$\epsilon$ 是随机误差, 即无法用线性关系解释的部分</li>
</ul>
<p>对于多变量:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>Y &#x3D; \beta_0 + \beta_1 X_1 + \beta_2 X_2 + … + \beta_p X_p + \epsilon<br>\end{aligned}<br>}<br>$$</p>
<p>直白点为:</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">target</span> = weight_1 * feature_1 + weight_2 * feature_2 + bias<br></code></pre></td></tr></table></figure>

<p>Linear Regression 模型在学习中主要就是调整 <code>weight_1</code>, <code>weight_2</code>, <code>bias</code> 等来匹配 <code>target</code>. 这些 <code>weight</code> 也称 regression coefficient (回归系数).</p>
<p>一般需要把时间转换为 time-step 的形式, 比如:</p>
<p><img src="/../img/machine-learning-time-dummy-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>该方式称 time dummy (此时 time-step 作为横轴)</li>
</ul>
<p>使用 Time Series 时, 可以把目标变量的历史值作为一个 feature, 比如:</p>
<p><img src="/../img/machine-learning-lag-feature-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>这里用前一天的 hardcover 量作为新 feature (此时 lag feature 作为横轴)</li>
</ul>
<h1 id="Kaggle"><a href="#Kaggle" class="headerlink" title="Kaggle"></a>Kaggle</h1><p>可以下载 <code>kaggle-api</code> 来在终端提交 kaggle competition 内容:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">yay -S kaggle-api<br></code></pre></td></tr></table></figure>

<p>或者用 <code>pip</code>:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">pip install kaggle<br></code></pre></td></tr></table></figure>

<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>在 <code>kaggle.com/settings</code> 中创建自己的 API Key 并下载复制到 <code>~/.kaggle/kaggle.json</code>.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">mkdir</span> ~/.kaggle<br><span class="hljs-built_in">mv</span> ~/Downloads/kaggle.json ~/.kaggle<br><span class="hljs-built_in">chmod</span> 600 ~/.kaggle/kaggle.json<br></code></pre></td></tr></table></figure>

<h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h2><h3 id="列出可用数据集"><a href="#列出可用数据集" class="headerlink" title="列出可用数据集"></a>列出可用数据集</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">kaggle datasets list -s <span class="hljs-string">&quot;titanic&quot;</span><br></code></pre></td></tr></table></figure>

<h3 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">kaggle datasets download -d <span class="hljs-string">&quot;username/dataset-name&quot;</span> --unzip  <span class="hljs-comment"># 解压文件</span><br></code></pre></td></tr></table></figure>

<h3 id="下载竞赛数据"><a href="#下载竞赛数据" class="headerlink" title="下载竞赛数据"></a>下载竞赛数据</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">kaggle competitions download -c <span class="hljs-string">&quot;titanic&quot;</span><br></code></pre></td></tr></table></figure>

<h3 id="提交竞赛结果"><a href="#提交竞赛结果" class="headerlink" title="提交竞赛结果"></a>提交竞赛结果</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">kaggle competitions submit -c titanic -f submission.csv -m <span class="hljs-string">&quot;Message&quot;</span><br></code></pre></td></tr></table></figure>

<h3 id="查看竞赛排行榜"><a href="#查看竞赛排行榜" class="headerlink" title="查看竞赛排行榜"></a>查看竞赛排行榜</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">kaggle competitions leaderboard -c <span class="hljs-string">&quot;titanic&quot;</span> --show<br></code></pre></td></tr></table></figure>

<h1 id="常见激活函数的求导过程"><a href="#常见激活函数的求导过程" class="headerlink" title="常见激活函数的求导过程"></a>常见激活函数的求导过程</h1><h2 id="sigmoid-函数"><a href="#sigmoid-函数" class="headerlink" title="sigmoid 函数"></a>sigmoid 函数</h2><p>Sigmoid 将输入映射到 <code>(0,1)</code> 区间, 常用于二分类问题的输出层:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>f(x) &#x3D; \sigma(x) &#x3D; \frac{1}{1 + e^{-x}}<br>\end{aligned}<br>}<br>$$</p>
<p>求导:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>f(x) &#x3D; (1 + e^{-x})^{-1} \newline~ \newline<br>let\ u &#x3D; 1 + e^{-x}, f(u) &#x3D; u^{-1} \newline~ \newline<br>\frac{df}{dx} &#x3D; \frac{df}{du} \times \frac{du}{dx} \newline~ \newline<br>\frac{df}{du} &#x3D; - u^{-2} &#x3D; - (1 + e^{-x})^{-2} \newline~ \newline<br>\frac{du}{dx} &#x3D; - e^{-x} \newline~ \newline<br>\frac{df}{dx} &#x3D; \frac{e^{-x}}{(1 + e^{-x})^2}<br>\end{aligned}<br>}<br>$$</p>
<p>可进行代数变换简化:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>1 - f(x) &#x3D; 1 - \frac{1}{1 + e^{-x}} &#x3D; \frac{e^{-x}}{1 + e^{-x}} \newline~ \newline<br>\frac{df}{dx} &#x3D; \frac{e^{-x}}{(1 + e^{-x})^2} &#x3D; \frac{1}{1 + e^{-x}} \cdot \frac{e^{-x}}{1 + e^{-x}} &#x3D; f(x) \cdot (1 - f(x)) \newline~ \newline<br>f’(x) &#x3D; f(x) (1 - f(x))<br>\end{aligned}<br>}<br>$$</p>
<h2 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h2><p>Tanh 函数将输入映射到 <code>(-1,1)</code> 区间, 以 0 为中心, 通常比 sigmoid 表现好:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>f(x) &#x3D; tanh(x) &#x3D; \frac{e^x - e^{-x}}{e^x + e^{-x}} &#x3D; \frac{sinh(x)}{cosh(x)}<br>\end{aligned}<br>}<br>$$</p>
<p>求导:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\frac{d}{dx} sinh(x) &#x3D; cosh(x) \newline~ \newline<br>\frac{d}{dx} cosh(x) &#x3D; sinh(x) \newline~ \newline<br>f’(x) &#x3D; \frac{cosh(x) \cdot cosh(x) - sinh(x) \cdot sinh(x)}{cosh^2(x)} \newline~ \newline<br>&#x3D; \frac{cosh^2(x) - sinh^2(x)}{cosh^2(x)} &#x3D; sech^2(x)<br>\end{aligned}<br>}<br>$$</p>
<p>同样可进行代换:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>1 - f(x^2) &#x3D; 1 - tanh^2(x) &#x3D; sech^2(x) \newline~ \newline<br>f’(x) &#x3D; 1 - f(x)^2<br>\end{aligned}<br>}<br>$$</p>
<h2 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h2><p>ReLU 截断输入的负数部分:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>f(x) &#x3D; max(0, x) &#x3D; \begin{cases}<br>x,\ if\ x &gt; 0 \newline~ \newline<br>0,\ if\ x \le 0<br>\end{cases}<br>\end{aligned}<br>}<br>$$</p>
<p>求导:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>x &gt; 0, f(x) &#x3D; x, f’(x) &#x3D; 1 \newline~ \newline<br>x &lt; 0, f(x) &#x3D; 0, f’(x) &#x3D; 0 \newline~ \newline<br>f’(x) &#x3D; \begin{cases}<br>1,\ if\ x &gt; 0 \newline~ \newline<br>0,\ if\ x \le 0<br>\end{cases}<br>\end{aligned}<br>}<br>$$</p>
<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>Softmax 函数将一个向量映射为另一个向量, 其输出元素值在 <code>(0,1)</code> 之间且和为 <code>1</code>, 通常用于多分类问题的输出层:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>S_i &#x3D; \frac{e^{z_i}}{\sum_{j&#x3D;1}^K e^{z_j}}<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$S_i$ 是输出向量的第 <code>i</code> 个元素, $z_i$ 是输入向量的第 <code>i</code> 个元素, <code>K</code> 是向量维度 (类别数)</li>
</ul>
<p>求导, 由于 $S_i$ 对每个输入 $z_j$ 都有依赖, 因此需要对每个输入求偏导, 这也分两种情况:</p>
<ol>
<li>$i &#x3D; j$, 求自身输入的偏导</li>
</ol>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>S_i &#x3D; \frac{e^{z_i}}{\sum_k e^{z_k}} \newline~ \newline<br>C &#x3D; \sum_k e^{z_k} (C 是包含 z_i 的函数) \newline~ \newline<br>S_i &#x3D; e^{z_i} \cdot C^{-1} \newline~ \newline<br>\frac{\partial S_i}{\partial z_i} &#x3D; \frac{\partial}{\partial z_i} (e^{z_i}) \cdot C^{-1} + e^{z_i} \cdot \frac{\partial}{\partial z_i} (C^{-1}) \newline~ \newline<br>&#x3D; e^{z_i} \cdot C^{-1} + e^{z_i} \cdot (-C^{-2} \cdot \frac{\partial C}{\partial z_i}) \newline~ \newline<br>\because \frac{\partial C}{\partial z_i} &#x3D; \frac{\partial}{\partial z_i} (\sum_k e^{z_k}) &#x3D; e^{z_i} \newline~ \newline<br>\frac{\partial S_i}{\partial z_i} &#x3D; e^{z_i} \cdot C^{-1} - e^{z_i} \cdot C^{-2} \cdot e^{z_i} \newline~ \newline<br>&#x3D; \frac{e^{z_i}}{C} - \frac{(e^{z_i})^2}{C^2} \newline~ \newline<br>&#x3D; S_i - S_i^2 \newline~ \newline<br>&#x3D; S_i (1 - S_i)<br>\end{aligned}<br>}<br>$$</p>
<ol start="2">
<li>$i \ne j$, 对其他输入的偏导<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>S_i &#x3D; e^{z_i} \cdot C^{-1} \newline~ \newline<br>\frac{\partial S_i}{\partial z_j} &#x3D; e^{z_i} \cdot \frac{\partial}{\partial z_j} (C^{-1}) \newline~ \newline<br>&#x3D; e^{z_i} \cdot (-C^{-2} \cdot \frac{\partial C}{\partial z_j}) \newline~ \newline<br>\because \frac{\partial C}{\partial z_j} &#x3D; e^{z_j} \newline~ \newline<br>\frac{\partial S_i}{\partial z_j} &#x3D; e^{z_i} \cdot (- C^{-2} \cdot e^{z_j}) \newline~ \newline<br>&#x3D; - \frac{e^{z_i}}{C} \cdot \frac{e^{z_j}}{C} \newline~ \newline<br>&#x3D; - S_i S_j<br>\end{aligned}<br>}<br>$$</li>
</ol>
<p>最终导数:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\frac{\partial S_i}{\partial z_j} &#x3D; \begin{cases}<br>S_i (1 - S_j),\ if\ i&#x3D;j \newline~ \newline<br>-S_i S_j,\ if\ i \ne j<br>\end{cases}<br>\end{aligned}<br>}<br>$$</p>
<p>该表达式可以写为 <code>Jacobian matrix</code>:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>J &#x3D; diag(p) - p \cdot p^T<br>\end{aligned}<br>}<br>$$</p>
<h1 id="数值溢出-x2F-下溢问题和解决"><a href="#数值溢出-x2F-下溢问题和解决" class="headerlink" title="数值溢出&#x2F;下溢问题和解决"></a>数值溢出&#x2F;下溢问题和解决</h1><h2 id="溢出"><a href="#溢出" class="headerlink" title="溢出"></a>溢出</h2><p>在 cross entropy 或 softmax 等实现中, 需要用到指数运算如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X_exp = np.exp(X)<br>X_exp_sum = X_exp.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>

<p>当 <code>X</code> 的值较大时 (比如 <code>&gt; 709</code>), <code>np.exp(X)</code> 会产生 <code>inf</code> 导致:</p>
<ul>
<li><code>X_exp_sum</code> 也变成 <code>inf</code></li>
<li>最终结果 <code>X_exp / X_exp_sum</code> 变成 <code>nan</code></li>
</ul>
<h2 id="下溢"><a href="#下溢" class="headerlink" title="下溢"></a>下溢</h2><p>当 <code>X</code> 的值很小且为负数时 (比如 <code>&lt; -745</code>), <code>np.exp(X)</code> 会下溢为 <code>0</code>, 导致:</p>
<ul>
<li>除法时出现 <code>0/0</code> 的情况, 产生 <code>nan</code> 值</li>
</ul>
<h2 id="log-0-问题"><a href="#log-0-问题" class="headerlink" title="log(0) 问题"></a>log(0) 问题</h2><p>交叉熵中用:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">losses[i] = -np.log(y_hat[i, y[i]])<br></code></pre></td></tr></table></figure>
<ul>
<li>如果 <code>y_hat[i, y[i]] = 0</code>, 会导致 <code>log(0) = -inf</code> 产生 <code>nan</code> 值</li>
</ul>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><h3 id="对于-Softmax-的数值稳定"><a href="#对于-Softmax-的数值稳定" class="headerlink" title="对于 Softmax 的数值稳定"></a>对于 <code>Softmax</code> 的数值稳定</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X_shifted = X - np.<span class="hljs-built_in">max</span>(X, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># 关键步骤！</span><br>X_exp = np.exp(X_shifted)  <span class="hljs-comment"># 现在最大值是0，exp(0)=1，不会溢出</span><br></code></pre></td></tr></table></figure>

<p>数学原理:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>softmax(x_i) &#x3D; \frac{e^{x_i}}{\sum_j e^{x_j}} &#x3D; \frac{e^{-C} \cdot e^{x_i}}{e^{-C} \cdot \sum_je^{x_k}} &#x3D; \frac{e^{x_i - C}}{\sum_j e^{x_j - C}} \newline~ \newline<br>\because C &#x3D; max(x_j) \newline~ \newline<br>x_i - C \le 0 \newline~ \newline<br>e^{x_i - C} \le e^0 &#x3D; 1 (分子分母不会溢出) \newline~ \newline<br>e^{C - C} &#x3D; e^0 &#x3D; 1, 分母至少比 1 大 (不会下溢为 0)<br>\end{aligned}<br>}<br>$$</p>
<p>不稳定的实现示例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 危险的大数值</span><br>X = np.array([<span class="hljs-number">1000</span>, <span class="hljs-number">1001</span>, <span class="hljs-number">1002</span>])<br><br><span class="hljs-comment"># 不稳定的计算</span><br><span class="hljs-keyword">try</span>:<br>    exp_X = np.exp(X)<br>    softmax_unsafe = exp_X / exp_X.<span class="hljs-built_in">sum</span>()<br>    <span class="hljs-built_in">print</span>(softmax_unsafe)<br><span class="hljs-keyword">except</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;溢出错误！&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>输出:</p>
<figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arcade">[<span class="hljs-literal">nan</span> <span class="hljs-literal">nan</span> <span class="hljs-literal">nan</span>]<br></code></pre></td></tr></table></figure>

<p>稳定输出的例子:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">X = np.array([<span class="hljs-number">1000</span>, <span class="hljs-number">1001</span>, <span class="hljs-number">1002</span>])<br>C = np.<span class="hljs-built_in">max</span>(X)<br>X_shifted = X - C<br>exp_X_shifted = np.exp(X_shifted)<br>softmax_safe = exp_X_shifted / exp_X_shifted.<span class="hljs-built_in">sum</span>()<br><span class="hljs-built_in">print</span>(softmax_safe)<br></code></pre></td></tr></table></figure>

<p>输出:</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dns">[<span class="hljs-number">0.09003057</span> <span class="hljs-number">0.24472847</span> <span class="hljs-number">0.66524096</span>]<br></code></pre></td></tr></table></figure>

<h3 id="Cross-Entropy-数值稳定"><a href="#Cross-Entropy-数值稳定" class="headerlink" title="Cross Entropy 数值稳定"></a>Cross Entropy 数值稳定</h3><p>对于 cross entropy:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>crossEntropy &#x3D; -\log p(y[i])<br>\end{aligned}<br>}<br>$$</p>
<p>需要防止:</p>
<ul>
<li><code>log(0)</code>, 值为 <code>-inf</code></li>
<li><code>log(1)</code>, 值为 <code>0</code></li>
</ul>
<p>稳定的实现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size = y_hat.shape[<span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># 双重保护：确保输入已经是数值稳定的</span><br>y_hat_clipped = np.clip(y_hat, <span class="hljs-number">1e-12</span>, <span class="hljs-number">1.0</span> - <span class="hljs-number">1e-12</span>)<br><br><span class="hljs-comment"># 使用向量化操作，避免循环</span><br>correct_probs = y_hat_clipped[np.arange(batch_size), y]<br>losses = -np.log(correct_probs)<br></code></pre></td></tr></table></figure>

<ul>
<li><code>np.clip(array, min_value, max_value)</code>, 将数组中的值限制在指定范围内, 超出范围的值会被裁剪到边界值</li>
<li>这里的 <code>1e-12</code> 是一个很小的数, 用于近似 <code>0</code>; <code>1.0 - 1e-12</code> 用于近似 <code>1</code></li>
</ul>
<h1 id="Forward-pass-中每层的输入维度"><a href="#Forward-pass-中每层的输入维度" class="headerlink" title="Forward pass 中每层的输入维度"></a>Forward pass 中每层的输入维度</h1><p>每层的输入维度的第一层始终是 <code>batch_size</code>, 比如最开始输入 <code>X</code> 的维度是 <code>batch_size x feature_num</code>:</p>
<ul>
<li>经过第一层全连接层 <code>W_1</code> 的维度得是 <code>feature_num x hidden_num</code>, 此时的输出是 <code>batch_size x hidden_num</code></li>
<li>经过第二层全连接层 <code>W_2</code> 的维度得是 <code>hidden_num x hidden_num</code>, 此时的输出还是 <code>batch_size x hidden_num</code></li>
<li>以此类推</li>
</ul>
<h1 id="Backward-pass-中往回传递过程中梯度的维度"><a href="#Backward-pass-中往回传递过程中梯度的维度" class="headerlink" title="Backward pass 中往回传递过程中梯度的维度"></a>Backward pass 中往回传递过程中梯度的维度</h1><p>由于 Loss 往往只是一个标量, 因此:</p>
<ul>
<li>Loss 与前向输出的梯度: $\frac{\partial L}{\partial Y}$ 的维度就是前向输出的维度</li>
<li>Loss 与前向输入的梯度: $\frac{\partial L}{\partial X}$ 的维度就是前向输入的维度</li>
<li>Loss 与权重矩阵的梯度: $\frac{\partial L}{\partial W}$ 的维度就是权重矩阵的维度</li>
<li>Loss 与偏置向量的梯度: $\frac{\partial L}{\partial b}$ 的维度就是偏置向量的维度</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Machine-Learning-入门</div>
      <div>http://example.com/2025/07/02/Machine-Learning-入门/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Jie</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年7月2日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/07/03/Python-sklearn-%E5%BA%93/" title="Python-sklearn-库">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Python-sklearn-库</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/07/01/Python-pandas-%E5%BA%93/" title="Python-pandas-库">
                        <span class="hidden-mobile">Python-pandas-库</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'zKurisu/comments-utterances');
      s.setAttribute('issue-term', 'pathname');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Jie</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Orkarin</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
