

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/wallhaven-j5kjgy_1920x1080.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Jie">
  <meta name="keywords" content="">
  
    <meta name="description" content="B 站讲解Arxiv mamba 论文Vision Mamba 论文Visual Guide to mamba and state space modelsMamba GithubState Space Mpdels 介绍Mamba 模型源自论文: “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”, 是一种序列模型">
<meta property="og:type" content="article">
<meta property="og:title" content="Mamba-模型">
<meta property="og:url" content="http://example.com/2025/09/21/Mamba-%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="B 站讲解Arxiv mamba 论文Vision Mamba 论文Visual Guide to mamba and state space modelsMamba GithubState Space Mpdels 介绍Mamba 模型源自论文: “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”, 是一种序列模型">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/mamba-compared-to-rnn-and-transformer.png">
<meta property="og:image" content="http://example.com/img/">
<meta property="og:image" content="http://example.com/img/mamba-selective-input-example.png">
<meta property="og:image" content="http://example.com/img/mamba-copying-task-example.png">
<meta property="og:image" content="http://example.com/img/mamba-selective-copying-example.png">
<meta property="og:image" content="http://example.com/img/mamba-induction-heads-task-example.png">
<meta property="og:image" content="http://example.com/img/mamba-scan-operation-sequential-computation-example.png">
<meta property="og:image" content="http://example.com/img/parallel-scan-example.jpg">
<meta property="og:image" content="http://example.com/img/state-space-model-convolution-kernel-for-2d-example.png">
<meta property="og:image" content="http://example.com/img/state-space-model-1d-convolution-kernel-example.png">
<meta property="og:image" content="http://example.com/img/state-space-model-convolution-kernel-len-as-input.png">
<meta property="og:image" content="http://example.com/img/state-space-model-sequence-with-padding.png">
<meta property="og:image" content="http://example.com/img/state-space-model-function-of-matrix-A.png">
<meta property="og:image" content="http://example.com/img/state-space-model-bad-selection-of-A.png">
<meta property="og:image" content="http://example.com/img/lssl-number-of-parameters.png">
<meta property="og:image" content="http://example.com/img/mamba-gated-mlp-example.png">
<meta property="og:image" content="http://example.com/img/mamba-block-example.png">
<meta property="article:published_time" content="2025-09-21T02:28:28.000Z">
<meta property="article:modified_time" content="2025-09-23T00:54:39.136Z">
<meta property="article:author" content="Jie">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/mamba-compared-to-rnn-and-transformer.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Mamba-模型 - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  

  

  



  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Jie</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/SteinsGate_all.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Mamba-模型"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-09-21 10:28" pubdate>
          2025年9月21日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          10k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          84 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Mamba-模型</h1>
            
            
              <div class="markdown-body">
                
                <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1KH4y1W7cm?spm_id_from=333.788.videopod.sections&vd_source=bc8ddbb1a08707dc809c3fd9bb85290d">B 站讲解</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2312.00752">Arxiv mamba 论文</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.09417">Vision Mamba 论文</a><br><a target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state">Visual Guide to mamba and state space models</a><br><a target="_blank" rel="noopener" href="https://github.com/state-spaces/mamba">Mamba Github</a><br><a target="_blank" rel="noopener" href="https://tinkerd.net/blog/machine-learning/state-space-models/#hungry-hungry-hippos-h3">State Space Mpdels</a></p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Mamba 模型源自论文: “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”, 是一种序列模型, 旨在解决传统 Transformer 架构在长序列模型建模时计算效率低, 内存占用高的问题.</p>
<p><img src="/../img/mamba-compared-to-rnn-and-transformer.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="对比-Transformer"><a href="#对比-Transformer" class="headerlink" title="对比 Transformer"></a>对比 Transformer</h1><p>Transformer 的空间和时间复杂度都是 $O(n^2)$.</p>
<h1 id="对比-RNN"><a href="#对比-RNN" class="headerlink" title="对比 RNN"></a>对比 RNN</h1><p>RNN 能够以 $O(n)$ 时间复杂度处理序列数据, 但是处理长序列容易遗忘.</p>
<h1 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h1><h2 id="Sequence-modelling"><a href="#Sequence-modelling" class="headerlink" title="Sequence modelling"></a>Sequence modelling</h2><p>Sequence modelling 指, 将一个输入序列映射到一个输出序列. Discrete 或 continuous 序列都行, 比如:</p>
<ul>
<li>text 是 discrete sequence</li>
<li>audio signals 是 continuous sequence</li>
</ul>
<p>通常用 <code>context window</code>, <code>training compute complexity</code>, <code>parallelizability</code> 和 <code>inference compute complexity</code> 来衡量 sequence model 的性能:</p>
<table>
<thead>
<tr>
<th>Properties</th>
<th>RNN</th>
<th>CNN</th>
<th>Transformers</th>
</tr>
</thead>
<tbody><tr>
<td>Context Window</td>
<td>Infinite context window</td>
<td>context window dependent on kernel size</td>
<td>infinite context window</td>
</tr>
<tr>
<td>Training Complexity</td>
<td>$O(N)$</td>
<td>Training dependents on kernel size</td>
<td>$O(N^2)$</td>
</tr>
<tr>
<td>Parallelizability</td>
<td>Not parallelizable</td>
<td>Easily parallelizable</td>
<td>Easily parallelizable</td>
</tr>
<tr>
<td>Inference Complexity</td>
<td>$O(1)$</td>
<td>Inference dependent on kernel size</td>
<td>O(N) after KV Caching</td>
</tr>
</tbody></table>
<p>(注意这里的 inference 应该是单步计算, 而不是整个序列)</p>
<p>Mamba 想要达到:</p>
<ul>
<li>Transformer 的并行训练</li>
<li>RNN 的线性时间复杂度</li>
<li>RNN 对每个 token 的常数时间推理速度</li>
</ul>
<h2 id="State-Space"><a href="#State-Space" class="headerlink" title="State Space"></a>State Space</h2><p>State space, 状态空间, 用来描述一个 dynamic system 的内部所有可能的隐藏状态.</p>
<h2 id="State-Space-Model"><a href="#State-Space-Model" class="headerlink" title="State Space Model"></a>State Space Model</h2><p>State Space Model 描述一个系统接受输入, 产生输出并维持一些内部状态. 系统的输出取决于输入和当前状态:</p>
<p><img src="/../img/" srcset="/img/loading.gif" lazyload></p>
<p>数学表示通常为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>x_t &#x3D; f(x_{t-1}, u_t) (状态转移方程) \newline~ \newline<br>y_t &#x3D; g(x_t) (观测方程)<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$t$ 表示时间</li>
<li>$x_t$, 状态 (State), 表示系统在时间 $t$ 的隐藏内部情况</li>
<li>$u_t$, 输入 (Input), 表示外部对系统的影响</li>
<li>$y_t$, 输出 (Output), 表示我们实际观测到的东西</li>
<li>$f,g$: 函数 (Function), 描述状态如何更新 (<code>f</code>) 和输出如何生成 (<code>g</code>)</li>
</ul>
<p>一个线性 SSM 的示例:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>x_t &#x3D; Ax_{t-1} + Bu_t \newline~ \newline<br>y_t &#x3D; C x_t + D u_t<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>这里状态的更新和输出的计算都是线性表达式</li>
<li>可以把 $D u_t$ 视作 skip connection</li>
</ul>
<p>这里的 $A, B, C, D$ 通常称 State Space Model Parameters:</p>
<ul>
<li>$A$: the state transition matrix</li>
<li>$B$: the input matrix</li>
<li>$C$: the output matrix</li>
<li>$D$: the feedthrough matrix</li>
</ul>
<h2 id="S4"><a href="#S4" class="headerlink" title="S4"></a>S4</h2><p>S4, Structured State Space Sequential Model, 是一种高效处理长序列的 SSM 架构, 核心创新是将 SSM 结构化, 以更好结合现代深度学习框架, 并利用硬件加速.</p>
<h2 id="Selectively-Retain-Information"><a href="#Selectively-Retain-Information" class="headerlink" title="Selectively Retain Information"></a>Selectively Retain Information</h2><p>Selectively Retain Information 指选择性压缩输入的信息. 比如输入一段文本, 其中会有一些没有实际意义的信息, 将其忽略. 从而能够动态决定哪些输入信息需要存储到状态中.</p>
<p><img src="/../img/mamba-selective-input-example.png" srcset="/img/loading.gif" lazyload></p>
<p>Selective 机制的核心是 “动态选择”, 模型不是平等处理所有输入信息, 而是根据输入的重要性, 动态决定哪些信息需要被重点关注和处理. 这样能让模型在处理长序列时聚焦关键信息.</p>
<p>这与 Transformer 中计算注意力权重类似, 但 Attention 的计算复杂度是 $O(n^2)$.</p>
<h2 id="Subquadratic-Time-architectures"><a href="#Subquadratic-Time-architectures" class="headerlink" title="Subquadratic-Time architectures"></a>Subquadratic-Time architectures</h2><p>Subquadratic-time architectures, 亚二次时间复杂度架构, 指计算复杂度低于传统 $O(n^2)$ 的模型架构, 用于高效处理长序列数据, 如长文本, 时间序列, 视频帧序列.</p>
<p>Transformer 的 self-attention 的计算复杂度是 $O(n^2)$ (n 是序列长度), 因为每个 token 都要与其他所有 token 计算注意力权重. 当 $n$ 很大时, 计算两和内存占用会急剧上升, 导致训练和推理速度极慢.</p>
<p>如果计算的时间复杂度为 $O(n logn)$ 或 $O(n)$, 能够在不显著损失性能的情况下, 大幅提升长序列处理效率.</p>
<h2 id="Linear-time-invariance"><a href="#Linear-time-invariance" class="headerlink" title="Linear time invariance"></a>Linear time invariance</h2><p>Linear time invariance 系统包含两个性质:</p>
<ul>
<li>Linearity</li>
<li>Time-invariance</li>
</ul>
<p>一个系统是 Linearity 的需要满足两个条件:</p>
<ul>
<li><p>Additivity:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>若 x_1(t) \rightarrow y_1(t), x_2(t) \rightarrow y_2(t) \newline~ \newline<br>则 x_1(t) + x_2(t) \rightarrow y_1(t) + y_2(t)<br>\end{aligned}<br>}<br>$$</p>
</li>
<li><p>Homogeneity (齐次性)<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>若 x(t) \rightarrow y(t), 则 a \dot x(t) \rightarrow a \cdot y(t)<br>\end{aligned}<br>}<br>$$</p>
</li>
</ul>
<p>综合起来就是:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>若 x_1(t) \rightarrow y_1(t), x_2(t) \rightarrow y_2(t) \newline~ \newline<br>则 a x_1(t) + bx_2(t) \rightarrow ay_1(t) + by_2(t)<br>\end{aligned}<br>}<br>$$</p>
<p>一个系统是 time invariant 的, 当且仅当系统的行为不随时间改变, 也就是说:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>若 x(t) \rightarrow y(t), 则 x(t - t_0) \rightarrow y(t - t_0)<br>\end{aligned}<br>}<br>$$</p>
<h2 id="Copying-任务"><a href="#Copying-任务" class="headerlink" title="Copying 任务"></a>Copying 任务</h2><p>Copying 任务用于练习模型对给定标记的简单记忆和再现能力, 其让模型看到一系列标记 (tokens), 其中一部分标记是需要被记住并在后续输出的, 这些标记按固定规则排列, </p>
<p><img src="/../img/mamba-copying-task-example.png" srcset="/img/loading.gif" lazyload></p>
<p>这里彩色标记需要记忆, 白色需要过滤, 黑色标记表示从这里开始再现. 模型需要有内容感知能力 (能够理解每个标记的含义和重要性)&lt; 能够记忆相关标记并过滤无关标记.</p>
<p>Selective Copying 任务是 Copying 任务的改进, 通过改变需要记忆的标记的位置来增加任务难度.</p>
<p><img src="/../img/mamba-selective-copying-example.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="Induction-heads-任务"><a href="#Induction-heads-任务" class="headerlink" title="Induction heads 任务"></a>Induction heads 任务</h2><p>Induction heads 任务用于解释大语言模型的上下文学习能力 (in-context learning), 需要模型根据输入的上下文信息, 在没有明确训练的情况下, 生成合适的输出.</p>
<p>Context-aware reasoning 指模型能理解输入的上下文信息, 包括标记的顺序, 周围标记的含义等.</p>
<p>模型要根据上下文信息, 判断在什么情况下应该产生什么样的输出:</p>
<p><img src="/../img/mamba-induction-heads-task-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>当黑色标记出现时, 输出合适内容 (这里是蓝色标记)</li>
</ul>
<h2 id="Kernel-fusion"><a href="#Kernel-fusion" class="headerlink" title="Kernel fusion"></a>Kernel fusion</h2><p>Kernel 在深度学习中, 通常指一个核心的计算单元或函数, 可以是一个矩阵乘法, 卷积操作或者其他复杂数学运算.</p>
<p>Kernel fusion 指一种优化计算的技术, 将多个独立的小核函数合并成一个大的核函数进行一次性计算, 减少计算过程中的内存访问开销和内核启动开销.</p>
<p>比如连续的矩阵乘法, 加法, 如果分开执行, 每次都会涉及数据的读取和写入带来较大开销. 可以将多个矩阵乘法操作合并成一个大的矩阵乘法, 一次性完成计算.</p>
<h2 id="Parallel-Scan"><a href="#Parallel-Scan" class="headerlink" title="Parallel Scan"></a>Parallel Scan</h2><p>Scan 在计算机科学中, 是一种对序列数据进行累积操作的过程. 例如, 对一个序列 $x_1,x_2,…,x_n$ 进行累加扫描, 就是计算:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>y_1 &#x3D; x_1, y_2 &#x3D; x_1 + x_2, y_3 &#x3D; x_1 + x_2 + x_3, …, \newline~ \newline<br>y_n &#x3D; x_1 + x_2 + … + x_n<br>\end{aligned}<br>}<br>$$</p>
<p>对应到 recurrence 计算里就是: 先计算 previous state 然后才能计算 current state:</p>
<p><img src="/../img/mamba-scan-operation-sequential-computation-example.png" srcset="/img/loading.gif" lazyload></p>
<p>这种方式难以进行并行运算.</p>
<p>Parallel scan 指, 将 scan 操作分解成多个可以同时进行的子任务, 可以将状态转移过程中的部分计算进行并行处理.</p>
<p><img src="/../img/parallel-scan-example.jpg" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>比如这里, 将每个计算分为 4 个部分:<ul>
<li>$x_0$ 是最开始的输入</li>
<li>$x_1$ 只需要计算 $x_0 + x_1$</li>
<li>$x_2$ 分为 $x_1 + x_2$ 和 $x_0 + (x_1 + x_2)$ 两部分</li>
<li>$x_3$ 分为 $x_2 + x_3$, $(x_0 + x_1) + (x_2 + x_3)$</li>
<li>$x_4$ 分为 $x_3 + x_4$, $(x_1 + x_2) + (x_3 + x_4)$, $x_0 + (x_1 + x_2) + (x_3 + x_4)$</li>
</ul>
</li>
<li>垂直的计算不能并行计算, 但是横向可以, 比如可以同时计算:<ul>
<li>$x_0 + x_1$, $x_1 + x_2$, $x_2 + x_3$, $x_3 + x_4$ 这些</li>
</ul>
</li>
</ul>
<h2 id="Recomputation"><a href="#Recomputation" class="headerlink" title="Recomputation"></a>Recomputation</h2><p>Recomputation, 是一种在深度学习中用于减少内存占用的技术.</p>
<p>在训练神经网络时, 通常需要保存中间层的激活值, 以便在反向传播时使用, 但这会占用大量内存.</p>
<p>Recomputation 的核心思想是, 在反向传播时, 不保留所有的中间激活值, 而是在需要时重新计算.</p>
<h2 id="FLOPs"><a href="#FLOPs" class="headerlink" title="FLOPs"></a>FLOPs</h2><p>FLOPs, Floating Point Operations Per Second, 每秒浮点运算数. 通常用来分析模型的计算复杂度. FLOP 则指 “总浮点运算数量”.</p>
<p>比如:</p>
<ul>
<li>Addition, <code>a + b</code> 算 1 FLOP</li>
<li>Multiplication, <code>a * b</code> 算 1 FLOP</li>
<li>Matrix multiplication, <code>A x B</code>, 其中 $A \in R^{B \times C}, B \in R^{C \times D}$, 简化为 $B \times C \times D$ FLOPs</li>
</ul>
<h2 id="离散化步长核心"><a href="#离散化步长核心" class="headerlink" title="离散化步长核心"></a>离散化步长核心</h2><p>原始的 SSM 是连续的时间微分方程:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\frac{dh(t)}{dt} &#x3D; A h(t) + B x(t)<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$A$ 是状态转移矩阵</li>
<li>$B$ 是输入投影矩阵</li>
</ul>
<p>在输入计算机系统时, 需要将连续方程转为离散形式:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>h_t &#x3D; f(h_{t-1}, x_t)<br>\end{aligned}<br>}<br>$$</p>
<p>从连续到离散的采样步长就是 $\Delta$, 采样时间间隔.</p>
<p>传统 SSM 的数学表达为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>h_t &#x3D; \overline{A} h_{t-1} + \overline{B} x_t \newline~ \newline<br>y_t &#x3D; C h_t<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$\overline{A}, \overline{B}$, 表示离散化后的参数</li>
<li>参数 $\overline{A}$, $\overline{B}$, $C$ 都是静态的, 也就是说: 对所有输入都平等处理. 因此无法根据输入 $x_t$ 动态调整, 导致模型无法区分重要&#x2F;无关信息, 让长序列建模能力受限</li>
</ul>
<p>Selective SSM 的创新在于构建了一个动态化离散步长:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\Delta_t &#x3D; softplus(Parameter + Linear(x_t))<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$x_t$ 是当前时间步输入</li>
<li>$Linear$ 是线性投影</li>
<li>$softplus$ 是激活函数, 确保 $\Delta_t &gt; 0$</li>
<li>$Linear(x_t)$ 这个部分, 确保 $\Delta_t$ 受输入影响</li>
</ul>
<p>$\Delta$ 可以用来:</p>
<ul>
<li>过滤信息:<ul>
<li>大 $\Delta_t$ ($&gt; 1$), 此时 $\overline{A} &#x3D; e^{\Delta A} \approx 0$ 会强制状态重置, 更新状态, 忘记历史, 相当于此时的输入很重要</li>
<li>小 $\Delta_t$ ($\approx 0.01$), $\overline{A} &#x3D; e^{\Delta A} \approx I + \Delta A$, 保持历史状态, 忽略噪声</li>
</ul>
</li>
</ul>
<p>比如句子 <code>The car __ on the mat</code> 的预测人物</p>
<ul>
<li><p>输入 “The”, $\Delta &#x3D; 0.01$, 这个词并不重要, 此时保留初始状态: $e^{0.01A} \approx 1 \rightarrow h_t &#x3D; h_{t-1}$</p>
</li>
<li><p>输入 “cat”, $\Delta &#x3D; 1.2$, 这个词很重要, 重置当前语义, $e^{1.2A} \approx 0 \rightarrow h_t &#x3D; B_t x_t$</p>
</li>
<li><p>调整计算路径:</p>
<ul>
<li>小 $\Delta$, 倾向于 convolution mode, 计算快但耗内存</li>
<li>大 $\Delta$, 倾向于 recurrent mode, 计算慢但省内存</li>
</ul>
</li>
</ul>
<h2 id="Zero-order-hold"><a href="#Zero-order-hold" class="headerlink" title="Zero-order hold"></a>Zero-order hold</h2><p><a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLUMWjy5jgHK0MLv6Ksf-NHi7Ur8NRNU4Z">求导过程</a></p>
<p>Zero-order hold 是一种经典的离散化方法, 其假设输入信号在离散时间步之间保持恒定.</p>
<p>数学表达为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>对任意 $t \in [k \Delta, (k+1) \Delta]$ \newline~ \newline<br>有 x(t) &#x3D; x_k<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$k$ 是时间步序号, $\Delta$ 是步长, $k \Delta$ 表示当前时间步, $(k+1) \Delta$ 表示下一个时间步</li>
<li>这里表示, $x(t)$ 在 $k\Delta$ 到 $(k+1) \Delta$ 这个时间段内的值都是 $x_k$</li>
</ul>
<p>假设有连续时间状态空间方程:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\frac{dh(t)}{dt} &#x3D; Ah(t) + Bx(t)<br>\end{aligned}<br>}<br>$$</p>
<p>该微分方程的解为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>h(\Delta) &#x3D; e^{A\Delta} h(0) + (\int_0^\Delta e^{A(\Delta - \tau)} d \tau) B x_k<br>\end{aligned}<br>}<br>$$</p>
<p>矩阵指数的性质有:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\int_0^\Delta e^{A\tau} d\tau &#x3D; A^{-1} (e^{A \Delta} - I)<br>\end{aligned}<br>}<br>$$</p>
<p>因此有:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\overline{A} &#x3D; e^{A\Delta} \newline~ \newline<br>\overline{B} &#x3D; A^{-1} (e^{A \Delta} - I) B<br>\end{aligned}<br>}<br>$$</p>
<h2 id="State-Expansion"><a href="#State-Expansion" class="headerlink" title="State Expansion"></a>State Expansion</h2><p>State Expansion 指, 在状态空间中, 通过增加隐藏状态 $h_t \in R^N$ 的维度 $N$ 来提升模型容量和表达能力的策略.</p>
<p>传统 SSM 的状态转移方程为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>h_t &#x3D; \overline{A} h_{t-1} + \overline{B} x_t<br>\end{aligned}<br>}<br>$$</p>
<p>当 $N$ 较小时, 状态 $h_t$ 的记忆容量有限, 难以捕捉长程复杂依赖.</p>
<p>通过参数矩阵升维:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\overline{A} \in R^{N \times N}, \overline{B} \in R^{N \times d}, C \in R^{d \times N}<br>\end{aligned}<br>}<br>$$</p>
<p>将输入 $x_t \in R^d$ 投影到高维状态空间 $R^N$.</p>
<p>根据 HIPPO 理论, 记忆时间常数 $\tau \approx \frac{N}{\log N}$:</p>
<ul>
<li>当 $N &#x3D; 64$, 可记忆约 <code>1000</code> 步依赖</li>
<li>当 $N &#x3D; 256$, 可记忆约 <code>10000</code> 步依赖</li>
</ul>
<h2 id="SSMs-as-Convolutional-Neural-network"><a href="#SSMs-as-Convolutional-Neural-network" class="headerlink" title="SSMs as Convolutional Neural network"></a>SSMs as Convolutional Neural network</h2><p>可以把 State Space Model 看作 CNN.</p>
<p>状态转移方程为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>x_t &#x3D; A x_{t - 1} + Bu_t<br>\end{aligned}<br>}<br>$$</p>
<p>假设初始状态 $x_{-1} &#x3D; 0$, 则有:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>x_0 &#x3D; A x_{-1} + B u_0 &#x3D; B u_0 \newline~ \newline<br>x_1 &#x3D; A x_0 + B u_1 &#x3D; AB u_0 + B u_1 \newline~ \newline<br>x_2 &#x3D; A x_1 + B u_2 &#x3D; A^2 B u_0 + A B u_1 + B u_2 \newline~ \newline<br>x_3 &#x3D; A x_2 + B u_3 &#x3D; A^3 B u_0 + A^2 B u_1 + A B u_2 + B u_3<br>\end{aligned}<br>}<br>$$</p>
<p>可以推出:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>x_t &#x3D; A^t B u_0 + A^{t-1} B u_1 + A^{t-2} B u_2 + … + B u_t<br>\end{aligned}<br>}<br>$$</p>
<p>假设有一个卷积核:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>K_t &#x3D; (A^t B, A^{t-1} B, A^{t-2}B, …, B)<br>\end{aligned}<br>}<br>$$</p>
<p>可以将 $x$ 的计算过程重写为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>x &#x3D; K * u<br>\end{aligned}<br>}<br>$$</p>
<p>一般情况下, 卷积核是 2D 的, 用作 filter:</p>
<p><img src="/../img/state-space-model-convolution-kernel-for-2d-example.png" srcset="/img/loading.gif" lazyload></p>
<p>对于 1D 的卷积核 (用在 1D 数据上):</p>
<p><img src="/../img/state-space-model-1d-convolution-kernel-example.png" srcset="/img/loading.gif" lazyload></p>
<p>在这里, 相当于卷积核的长度等于序列长度:</p>
<p><img src="/../img/state-space-model-convolution-kernel-len-as-input.png" srcset="/img/loading.gif" lazyload></p>
<p>这样只进行了一次运算, 也可以通过 padding 增加运算:</p>
<p><img src="/../img/state-space-model-sequence-with-padding.png" srcset="/img/loading.gif" lazyload></p>
<p>能这样并行运算的前提是该模型是 Time-Invariant 的, 参数 $A$, $B$ 与时间步 <code>t</code> 无关.</p>
<p>因为这里有:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>y &#x3D; Cx + Du \newline~ \newline<br>&#x3D; C(K * u) + Du \newline~ \newline<br>&#x3D; (CK * u) + Du<br>\end{aligned}<br>}<br>$$</p>
<p>可以把 $C$ 并到 $K$ 中:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>K_t &#x3D; (CA^tB, CA^{t-1}B, CA^{t-2}B, …, CB)<br>\end{aligned}<br>}<br>$$</p>
<h2 id="HiPPO"><a href="#HiPPO" class="headerlink" title="HiPPO"></a>HiPPO</h2><p>SSM2 中的 matrix A 用来决定那些 previous state 要传递到 current state 中:</p>
<p><img src="/../img/state-space-model-function-of-matrix-A.png" srcset="/img/loading.gif" lazyload></p>
<p>这里 state 的长度是固定的, 记为 $N$.</p>
<p>如果 $A$ 选取比较差, 可能会导致信息丢失, 比如一个极端的例子, 将 $A$ 取为全 <code>0</code>, 则会丢失全部 previous state:</p>
<p><img src="/../img/state-space-model-bad-selection-of-A.png" srcset="/img/loading.gif" lazyload></p>
<p>很多基于 SSMs 的语言模型都利用 HiPPO Matrix 来初始化 $A$:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>A_{nk} &#x3D; - \begin{cases}<br>(2n + 1)^{1&#x2F;2} (2k + 1)^{1&#x2F;2}\ if\ n &gt; k \newline~ \newline<br>n + 1\ if\ n &#x3D; k \newline~ \newline<br>0\ if\ n &gt; k \newline~ \newline<br>\end{cases}<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$n$ 是行号, $k$ 是列号</li>
</ul>
<p>代码示例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">hippo_matrix</span>(<span class="hljs-params">state_size</span>):<br>    <br>    <span class="hljs-comment"># Generate a square matrix with values (1, state_size+1) on the diagonal</span><br>    A = torch.diag(torch.arange(<span class="hljs-number">1</span>, state_size+<span class="hljs-number">1</span>, dtype=torch.<span class="hljs-built_in">float</span>))<br><br>    <span class="hljs-comment"># Function to generate the elements below the diagonal based on </span><br>    <span class="hljs-comment"># the row and column index</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">lower_triangle_elements</span>(<span class="hljs-params">n, k</span>):<br>        <span class="hljs-keyword">return</span> (<span class="hljs-number">2</span>*n + <span class="hljs-number">1</span>)**<span class="hljs-number">0.5</span> * (<span class="hljs-number">2</span>*k + <span class="hljs-number">1</span>)**<span class="hljs-number">0.5</span><br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, state_size):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i):<br>            A[i, j] = lower_triangle_elements(i, j)<br>    <br>    <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>*A<br><br>hippo_matrix(state_size=<span class="hljs-number">5</span>)<br><br><span class="hljs-comment"># tensor([[-1.0000, -0.0000, -0.0000, -0.0000, -0.0000],</span><br><span class="hljs-comment">#         [-1.7321, -2.0000, -0.0000, -0.0000, -0.0000],</span><br><span class="hljs-comment">#         [-2.2361, -3.8730, -3.0000, -0.0000, -0.0000],</span><br><span class="hljs-comment">#         [-2.6458, -4.5826, -5.9161, -4.0000, -0.0000],</span><br><span class="hljs-comment">#         [-3.0000, -5.1962, -6.7082, -7.9373, -5.0000]])</span><br></code></pre></td></tr></table></figure>

<h2 id="LSSL"><a href="#LSSL" class="headerlink" title="LSSL"></a>LSSL</h2><p>Linear State Space Layer, 的数学形式:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>x_t &#x3D; A{x_t} + B u_t \newline~ \newline<br>y_t &#x3D; C x_t + D u_t<br>\end{aligned}<br>}<br>$$</p>
<p>LSSL 中的参数量取决于:</p>
<ul>
<li>$N$, the state size, 即 state vector 的维度</li>
<li>$H$, the hidden size, 取决于输入的维度</li>
<li>$M$, the number of output channels</li>
</ul>
<p><img src="/../img/lssl-number-of-parameters.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="SILU"><a href="#SILU" class="headerlink" title="SILU"></a>SILU</h2><p>SILU, Sigmoid Linear Unit, 也称 switch 激活函数, 数学定义为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>SILU(x) &#x3D; x \cdot \sigma (x)<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$x$ 是输入值</li>
<li>$\sigma (x)$ 是 Sigmoid 函数:</li>
</ul>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\sigma (x) &#x3D; \frac{1}{1 + e^{-x}}<br>\end{aligned}<br>}<br>$$</p>
<p>SILU 将输入与一个 <code>0~1</code> 的权重相乘, 动态调整输入的强度.</p>
<h2 id="Gated-MLP"><a href="#Gated-MLP" class="headerlink" title="Gated MLP"></a>Gated MLP</h2><p><img src="/../img/mamba-gated-mlp-example.png" srcset="/img/loading.gif" lazyload></p>
<p>这里其实就是实现了 SILU activation:</p>
<ul>
<li>一路输入经 Linear Projection 调整维度</li>
<li>另一路输入先经 Linear Projection 调整维度, 再经 sigmoid 调整为权重</li>
<li>两路输入相乘: $x \cdot \sigma (x)$</li>
</ul>
<h1 id="Mamba-Block"><a href="#Mamba-Block" class="headerlink" title="Mamba Block"></a>Mamba Block</h1><p>Mamba block 是 Mamba 模型的基本计算单元, 示意图如下:</p>
<p><img src="/../img/mamba-block-example.png" srcset="/img/loading.gif" lazyload></p>
<p>包含的组件有:</p>
<ul>
<li>Linear Projection, 用于调整特征维度</li>
<li>Skip connections, 残差连接, 缓解梯度消失</li>
<li>A Selective State Space Machine, 建立长程依赖</li>
<li>1D Convolution layer, 对输入进行局部特征提取</li>
<li>SILU activation function, 门控机制, 增强非线性表达能力, 控制信息流动</li>
</ul>
<p>Mamba 由多个 Mamba block 堆叠而成:</p>
<figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mathematica"><span class="hljs-built_in">Input</span> → <span class="hljs-variable">Embedding</span> → <span class="hljs-punctuation">[</span><span class="hljs-variable">Mamba</span> <span class="hljs-built_in">Block</span> × <span class="hljs-built_in">N</span><span class="hljs-punctuation">]</span> → <span class="hljs-variable">Layer</span> <span class="hljs-built_in">Norm</span> → <span class="hljs-variable">Output</span> <span class="hljs-built_in">Head</span><br></code></pre></td></tr></table></figure>


                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Mamba-模型</div>
      <div>http://example.com/2025/09/21/Mamba-模型/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Jie</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年9月21日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/09/22/%E5%BE%AE%E7%A7%AF%E5%88%86%E6%8A%80%E5%B7%A7%E7%A7%AF%E7%B4%AF/" title="微积分技巧积累">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">微积分技巧积累</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/09/20/Binary-search-tree/" title="Binary-search-tree">
                        <span class="hidden-mobile">Binary-search-tree</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'zKurisu/comments-utterances');
      s.setAttribute('issue-term', 'pathname');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Jie</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Orkarin</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
