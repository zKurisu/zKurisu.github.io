

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/wallhaven-j5kjgy_1920x1080.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Jie">
  <meta name="keywords" content="">
  
    <meta name="description" content="有部分章节没做笔记, 因为在其他几篇 blog 中已经记录得比较详细了. 1 什么是机器学习定义: 让机器具有自主获取知识的能力, 而非将知识显式编程入机器. 最常用的两种机器学习算法:  Supervised learning, 监督式学习 Unsupervised learning, 非监督式学习  其他的如:  Recommender systems Reinforcement learni">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习-吴恩达-Notes">
<meta property="og:url" content="http://example.com/2025/08/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE-Notes/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="有部分章节没做笔记, 因为在其他几篇 blog 中已经记录得比较详细了. 1 什么是机器学习定义: 让机器具有自主获取知识的能力, 而非将知识显式编程入机器. 最常用的两种机器学习算法:  Supervised learning, 监督式学习 Unsupervised learning, 非监督式学习  其他的如:  Recommender systems Reinforcement learni">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/machine-learning-common-applications.png">
<meta property="og:image" content="http://example.com/img/five-features.png">
<meta property="og:image" content="http://example.com/img/machine-learning-find-boundery-in-classification.png">
<meta property="og:image" content="http://example.com/img/machine-learning-unsupervised-learning-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-terminology-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-terminology-example2.png">
<meta property="og:image" content="http://example.com/img/machine-learning-how-to-model-linear-regression.png">
<meta property="og:image" content="http://example.com/img/machine-learning-gradient-decent-make-sense.png">
<meta property="og:image" content="http://example.com/img/machine-learning-calculate-order-of-gradient-decent.png">
<meta property="og:image" content="http://example.com/img/machine-learning-gradient-decent-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-how-gradient-decent-works.png">
<meta property="og:image" content="http://example.com/img/machine-learning-advantage-of-vectorization.png">
<meta property="og:image" content="http://example.com/img/machine-learning-vectorization-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-linear-model-with-vector.png">
<meta property="og:image" content="http://example.com/img/machine-learning-feature-scale-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-mean-normalization-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-z-score-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-when-loss-function-converge.png">
<meta property="og:image" content="http://example.com/img/machine-learning-learning-rate-large.png">
<meta property="og:image" content="http://example.com/img/machine-learning-loss-function-gose-up.png">
<meta property="og:image" content="http://example.com/img/machine-learning-feature-engineering-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-polynomial-regression-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-linear-regression-for-tumor.png">
<meta property="og:image" content="http://example.com/img/machine-learning-sigmoid-function-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-decision-boundary-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-decision-boundary-to-linear-line.png">
<meta property="og:image" content="http://example.com/img/machine-learning-non-linear-decision-boundaries.png">
<meta property="og:image" content="http://example.com/img/machine-learning-cost-function-for-squared-error-cost-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-loss-function-for-logistic-regression.png">
<meta property="og:image" content="http://example.com/img/machine-learning-logistic-regression-loss-function.png">
<meta property="og:image" content="http://example.com/img/machine-learning-cost-function-for-logistic-regression.png">
<meta property="og:image" content="http://example.com/img/machine-learning-three-fit-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-three-fit-example-for-classification.png">
<meta property="og:image" content="http://example.com/img/machine-learning-collecting-more-training-examples.png">
<meta property="og:image" content="http://example.com/img/machine-learning-using-less-feature-for-fix-overfitting.png">
<meta property="og:image" content="http://example.com/img/machine-learning-regularizatioin-example.png">
<meta property="og:image" content="http://example.com/img/machine-learning-cost-function-with-regularization.png">
<meta property="og:image" content="http://example.com/img/machine-learning-regularization-example-for-all-parameters.png">
<meta property="og:image" content="http://example.com/img/machine-learning-regularization-effector-for-gradient-descent.png">
<meta property="og:image" content="http://example.com/img/machine-learning-regularized-logistic-regression.png">
<meta property="og:image" content="http://example.com/img/deep-learning-neuron-example2.png">
<meta property="og:image" content="http://example.com/img/deep-learning-why-neural-network-important.png">
<meta property="og:image" content="http://example.com/img/deep-learning-simple-neuron-network-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-with-multiple-hidden-layers.png">
<meta property="og:image" content="http://example.com/img/deep-learning-face-recognition-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-neuron-for-face-recognition.png">
<meta property="og:image" content="http://example.com/img/deep-learning-neural-network-layer1-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-neural-network-layer2-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-four-layers-neuron-network-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-example-layer3-calculation.png">
<meta property="og:image" content="http://example.com/img/deep-learning-forward-propagation-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-simple-tensor-flow-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-calculate-forward-prop-by-hand-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-other-activation-function-examples.png">
<meta property="og:image" content="http://example.com/img/deep-learning-why-ReLU-not-sigmoid.png">
<meta property="og:image" content="http://example.com/img/deep-learning-neuron-network-only-linear-model.png">
<meta property="og:image" content="http://example.com/img/deep-learning-multiclass-problem-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-how-softmax-calculate.png">
<meta property="og:image" content="http://example.com/img/deep-learning-loss-function-for-softmax.png">
<meta property="og:image" content="http://example.com/img/deep-learning-softmax-in-neuron-network-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-how-to-deal-with-multilabel-problem.png">
<meta property="og:image" content="http://example.com/img/deep-learning-adam-algorithm-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-detail-of-dense-layer-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-convolution-neuron-network-intro.png">
<meta property="og:image" content="http://example.com/img/deep-learning-convolutional-network-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-computation-graph-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-evaluate-with-test-set.png">
<meta property="og:image" content="http://example.com/img/deep-learning-from-curve-get-bias-variance-info.png">
<meta property="og:image" content="http://example.com/img/deep-learning-bias-and-variance-with-polynomial.png">
<meta property="og:image" content="http://example.com/img/deep-learning-try-to-find-a-good-lambda.png">
<meta property="og:image" content="http://example.com/img/deep-learning-how-to-use-baseline-model.png">
<meta property="og:image" content="http://example.com/img/deep-learning-learning-curves-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-learning-curve-for-high-bias-model.png">
<meta property="og:image" content="http://example.com/img/deep-learning-learning-rate-for-high-variance-model.png">
<meta property="og:image" content="http://example.com/img/deep-learning-bias-variance-tradeoff-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-neural-network-and-bias-variance-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-iterative-loop-of-ml-development.png">
<meta property="og:image" content="http://example.com/img/deep-learning-data-augmentation-example1.png">
<meta property="og:image" content="http://example.com/img/deep-learning-data-augmentation-example2.png">
<meta property="og:image" content="http://example.com/img/deep-learning-data-augmentation-of-speech-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-transfer-learning-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-full-cycle-of-a-machine-learning-project.png">
<meta property="og:image" content="http://example.com/img/deep-learning-deployment-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-confusion-matrix-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-precision-and-recall-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-threshold-with-precision-and-recall.png">
<meta property="og:image" content="http://example.com/img/deep-learning-f1-score-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-entropy-example1.png">
<meta property="og:image" content="http://example.com/img/deep-learning-information-gain-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-information-gain-example2.png">
<meta property="og:image" content="http://example.com/img/deep-learning-regression-with-decision-tree-example.png">
<meta property="og:image" content="http://example.com/img/">
<meta property="og:image" content="http://example.com/img/deep-learning-select-feature-for-regression-dt.png">
<meta property="og:image" content="http://example.com/img/deep-learning-tree-ensemble-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-random-forest-example.png">
<meta property="og:image" content="http://example.com/img/deep-learning-random-forest-example2.png">
<meta property="og:image" content="http://example.com/img/supervised-learning-training-set-example.png">
<meta property="og:image" content="http://example.com/img/unsupervised-learning-data-set-example.png">
<meta property="og:image" content="http://example.com/img/k-means-init-centers.png">
<meta property="og:image" content="http://example.com/img/k-means-calculate-which-group-belong-to.png">
<meta property="og:image" content="http://example.com/img/k-means-move-centroid-example.png">
<meta property="og:image" content="http://example.com/img/k-means-calculate-new-group.png">
<meta property="og:image" content="http://example.com/img/k-means-reach-to-end-example.png">
<meta property="og:image" content="http://example.com/img/k-means-algorithm-detail-definition.png">
<meta property="og:image" content="http://example.com/img/k-means-cost-function-example.png">
<meta property="og:image" content="http://example.com/img/k-means-random-initialization-example.png">
<meta property="og:image" content="http://example.com/img/k-means-initialization-with-bad-choice.png">
<meta property="og:image" content="http://example.com/img/k-means-random-initialization-multiple-times-example.png">
<meta property="og:image" content="http://example.com/img/k-means-what-is-the-right-value-of-k.png">
<meta property="og:image" content="http://example.com/img/k-means-using-elbow-method-to-find-k.png">
<meta property="og:image" content="http://example.com/img/k-means-find-k-by-purpose.png">
<meta property="og:image" content="http://example.com/img/anomaly-detection-intro-example.png">
<meta property="og:image" content="http://example.com/img/anomaly-detection-density-estimation-example.png">
<meta property="article:published_time" content="2025-08-11T11:02:33.000Z">
<meta property="article:modified_time" content="2025-08-15T11:45:12.067Z">
<meta property="article:author" content="Jie">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/machine-learning-common-applications.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>机器学习-吴恩达-Notes - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  

  

  



  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Jie</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/SteinsGate_all.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="机器学习-吴恩达-Notes"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-08-11 19:02" pubdate>
          2025年8月11日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          16k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          136 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">机器学习-吴恩达-Notes</h1>
            
            
              <div class="markdown-body">
                
                <p>有部分章节没做笔记, 因为在其他几篇 blog 中已经记录得比较详细了.</p>
<h1 id="1-什么是机器学习"><a href="#1-什么是机器学习" class="headerlink" title="1 什么是机器学习"></a>1 什么是机器学习</h1><p>定义: 让机器具有自主获取知识的能力, 而非将知识显式编程入机器.</p>
<p>最常用的两种机器学习算法:</p>
<ul>
<li>Supervised learning, 监督式学习</li>
<li>Unsupervised learning, 非监督式学习</li>
</ul>
<p>其他的如:</p>
<ul>
<li>Recommender systems</li>
<li>Reinforcement learning</li>
</ul>
<h1 id="2-监督学习"><a href="#2-监督学习" class="headerlink" title="2 监督学习"></a>2 监督学习</h1><p>Supervised learning 的核心在于从给定输入以及输出中学习. 通过训练模型来建立输入与输出之间的关系, 并利用该关系预测未知输入的输出.</p>
<p>常见应用:</p>
<p><img src="/../img/machine-learning-common-applications.png" srcset="/img/loading.gif" lazyload></p>
<p>监督学习可以分为两种类型：</p>
<ul>
<li>分类 (Classification): 在分类问题中, 目标是将输入数据分为预定义的类别或标签, 简单来说, 输出是有限的集合. 例如, 将电子邮件分为垃圾邮件和非垃圾邮件, 将图像识别为猫或狗等</li>
<li>回归 (Regression): 在回归问题中, 目标是预测一个连续的输出变量 (有无数种可能的数字). 例如, 预测房价, 股票价格等</li>
</ul>
<p>Regression (回归) 问题, 通过一些数据集来拟合正确的结果.</p>
<p>目的: 不仅处理一种 features, 而是处理无穷无尽的 features</p>
<p>这里处理 5 个 features.</p>
<p><img src="/../img/five-features.png" srcset="/img/loading.gif" lazyload></p>
<p>监督学习需要大量的标记数据，即输入和输出配对的数据，来训练模型。这些标记数据通常需要人工创建或收集，因此监督学习对数据集的要求较高.</p>
<p>也就是说, 先通过一组数据得到一个关系, 通过这个关系来预测新的未知的输入和输出.</p>
<p>Classification 问题很多时候需要找到类别间的分界线:</p>
<p><img src="/../img/machine-learning-find-boundery-in-classification.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="3-无监督学习"><a href="#3-无监督学习" class="headerlink" title="3 无监督学习"></a>3 无监督学习</h1><p>无监督学习（Unsupervised Learning）指在没有标签或类别信息的情况下, 通过训练模型来发现输入数据中的结构、模式和规律.</p>
<p>与监督学习不同，无监督学习的训练数据集中只包含输入数据, 没有对应的输出或标签信息.</p>
<p>Unsupervised learning 可用于 clustering, 将相似的数据 group 到一起, 如:</p>
<p><img src="/../img/machine-learning-unsupervised-learning-example.png" srcset="/img/loading.gif" lazyload></p>
<p>其他的应用如:</p>
<ul>
<li>Anomaly detection</li>
<li>Dimensionality reduction</li>
</ul>
<h1 id="6-Linear-regression-model"><a href="#6-Linear-regression-model" class="headerlink" title="6 Linear regression model"></a>6 Linear regression model</h1><p>Linear regression model 是一种 Supervised learning 算法, 核心思想是通过拟合一条直线来最小化预测值和真实值之间的误差, 建模连续型目标变量 (Y) 与一个或多个特征变量 X 之间的线性关系.</p>
<p>机器学习中常见的术语表示:</p>
<p><img src="/../img/machine-learning-terminology-example.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/../img/machine-learning-terminology-example2.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>training set</li>
<li>input variable&#x2F;feature</li>
<li>output variable&#x2F;target</li>
<li>prediction</li>
<li>number of training examples</li>
</ul>
<p>单特征的 linear regression model 数学表示为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>Y &#x3D; \beta_0 + \beta_1 X + \epsilon<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>$Y$: target variable</li>
<li>$X$: input variable&#x2F;feature</li>
<li>$\beta_0$: intercept</li>
<li>$\beta_1$: slope</li>
<li>$\epsilon$: 随机误差, noise</li>
</ul>
<p>多特征的 linear regression model 为:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>Y &#x3D; \beta_0 + \beta_1 X_1 + \beta_2 X_2  … + \beta_p X_p + \epsilon<br>\end{aligned}<br>}<br>$$</p>
<p>目标是找到一组参数, 使得预测值与真实值的误差最小.</p>
<h1 id="7-Cost-function"><a href="#7-Cost-function" class="headerlink" title="7 Cost function"></a>7 Cost function</h1><p>Cost function 可以用来衡量模型的表现如何.</p>
<p>单个样本的 <code>Error</code> 定义为 $\hat{y} - y$ (预测值减去真实值), 但一般需要用 $(\hat{y} - y)^2$. 全部样本则为 $\frac{1}{m}\sum_{i&#x3D;1}^m (\hat{y} - y)^2$, 这也称 squared error cost function.</p>
<p>比如建模 linear regression model, 需要有:</p>
<p><img src="/../img/machine-learning-how-to-model-linear-regression.png" srcset="/img/loading.gif" lazyload></p>
<p>Cost function 和 loss function 都能用于预测模型误差, 对比如下:</p>
<table>
<thead>
<tr>
<th><strong>对比维度</strong></th>
<th><strong>Loss Function（损失函数）</strong></th>
<th><strong>Cost Function（成本函数）</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>定义</strong></td>
<td>计算单个样本的预测误差</td>
<td>计算整个数据集（或 batch）的平均误差</td>
</tr>
<tr>
<td><strong>数学表达式</strong></td>
<td>$L(\hat{y}, y)$（单个样本）</td>
<td>$J(θ) &#x3D; (1&#x2F;N) Σ L(\hat{y}_i, y_i)$（所有样本）</td>
</tr>
<tr>
<td><strong>用途</strong></td>
<td>用于梯度计算（反向传播）</td>
<td>用于评估整体模型性能</td>
</tr>
<tr>
<td><strong>别名</strong></td>
<td>误差函数（Error Function）</td>
<td>目标函数（Objective Function）</td>
</tr>
<tr>
<td><strong>优化目标</strong></td>
<td>最小化单个样本的误差</td>
<td>最小化整体训练误差</td>
</tr>
</tbody></table>
<h1 id="10-Gradient-decent"><a href="#10-Gradient-decent" class="headerlink" title="10 Gradient decent"></a>10 Gradient decent</h1><p>假如有函数:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>J(w_1, w_2, …, w_n)<br>\end{aligned}<br>}<br>$$</p>
<p>需要找到一组参数满足目标:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>\underset{w_1,…,w_n}{min} J(w_1, w_2, …, w_n)<br>\end{aligned}<br>}<br>$$</p>
<p>Gradient descent algorithm 的思路是, 按梯度相反的方向更新参数, 意思是要计算每个参数的梯度并用于更新:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>w &#x3D; w - \alpha \frac{\partial}{\partial \w} J<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li><code>w</code> 是参数</li>
<li>$\alpha$ 是 learning rate, 一般是一个比较小的数字, 过小会收敛太慢, 过大或导致无法收敛或 diverge</li>
<li>$J$ 是 cost function</li>
</ul>
<p>比如一个包含两个参数的 cost function : $J(w,b)$, 参数更新为:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>w &#x3D; w - \alpha \frac{\partial}{\partial w} J(w,b) \newline~ \newline<br>b &#x3D; b - \alpha \frac{\partial}{\partial b} J(w,b)<br>\end{aligned}<br>}<br>$$</p>
<p>按梯度的反方向更新能保证 $J$ 在减小:</p>
<p><img src="/../img/machine-learning-gradient-decent-make-sense.png" srcset="/img/loading.gif" lazyload></p>
<p>注意实现过程中的计算顺序:</p>
<p><img src="/../img/machine-learning-calculate-order-of-gradient-decent.png" srcset="/img/loading.gif" lazyload></p>
<p>注意梯度下降容易陷入局部最优解:</p>
<p><img src="/../img/machine-learning-gradient-decent-example.png" srcset="/img/loading.gif" lazyload></p>
<p>一个计算示例:</p>
<p><img src="/../img/machine-learning-how-gradient-decent-works.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="18-向量化"><a href="#18-向量化" class="headerlink" title="18 向量化"></a>18 向量化</h1><p>Vectorization 的一大好处在于便于并行运算:</p>
<p><img src="/../img/machine-learning-advantage-of-vectorization.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/../img/machine-learning-vectorization-example.png" srcset="/img/loading.gif" lazyload></p>
<p>比如把多特征的 linear regresion model 写为向量形式:</p>
<p><img src="/../img/machine-learning-linear-model-with-vector.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="20-特征缩放"><a href="#20-特征缩放" class="headerlink" title="20 特征缩放"></a>20 特征缩放</h1><p>如果多个 feature sizes 之间的量级相差大, 会导致损失函数的 gradient descent 等高线呈 “狭长椭圆形”, 梯度下降会沿陡峭方向震荡, 收敛缓慢.</p>
<p>比如特征 $x_1$ 范围是 $[0,1]$, $x_2$ 范围是 $[0,1000]$, 此时损失函数对 $w_2$ 的梯度会远大于对 $w_1$ 的梯度, 即 $\frac{\partial J}{\partial w_2} \propto x_2$.</p>
<p>此时需要将 $x_2$ 缩放到较小范围内:</p>
<p><img src="/../img/machine-learning-feature-scale-example.png" srcset="/img/loading.gif" lazyload></p>
<p>常见方法如 mean normalization:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>x &#x3D; \frac{x - \mu}{max - min}<br>\end{aligned}<br>}<br>$$</p>
<p><img src="/../img/machine-learning-mean-normalization-example.png" srcset="/img/loading.gif" lazyload></p>
<p>还有 z-score:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>x &#x3D; \frac{x - \mu}{\sigma}<br>\end{aligned}<br>}<br>$$</p>
<p><img src="/../img/machine-learning-z-score-example.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="22-检查梯度下降是否收敛"><a href="#22-检查梯度下降是否收敛" class="headerlink" title="22 检查梯度下降是否收敛"></a>22 检查梯度下降是否收敛</h1><p>一般选择绘制 $J$ 为纵轴, <code>iteration</code> 次数为横轴的图像, 如:</p>
<p><img src="/../img/machine-learning-when-loss-function-converge.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="23-选择-learning-rate"><a href="#23-选择-learning-rate" class="headerlink" title="23 选择 learning rate"></a>23 选择 learning rate</h1><p>Learning rate 过大会导致 loss 震荡, 如:</p>
<p><img src="/../img/machine-learning-learning-rate-large.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>$w$ 增加一个很大的值或减少很大值</li>
</ul>
<p>如果 loss 一直增大, 可能是没有用 <code>-</code> 号:</p>
<p><img src="/../img/machine-learning-loss-function-gose-up.png" srcset="/img/loading.gif" lazyload></p>
<p>测试 learning rate 时, 可以是 <code>0.001</code>, <code>0.003</code>, <code>0.009</code>, <code>0.01</code>, <code>0.1</code>, <code>1</code> 这样.</p>
<h1 id="24-Feature-engineering"><a href="#24-Feature-engineering" class="headerlink" title="24 Feature engineering"></a>24 Feature engineering</h1><p>比如从已有 features 构建新的 features:</p>
<p><img src="/../img/machine-learning-feature-engineering-example.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="25-Polynomial-regression"><a href="#25-Polynomial-regression" class="headerlink" title="25 Polynomial regression"></a>25 Polynomial regression</h1><p>Polynomial regression 指引入非线性关系, 通过曲线来拟合.</p>
<p><img src="/../img/machine-learning-polynomial-regression-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>注意此时 feature scaling 比较重要, 不然容易溢出</li>
</ul>
<h1 id="26-Classification-问题"><a href="#26-Classification-问题" class="headerlink" title="26 Classification 问题"></a>26 Classification 问题</h1><p>Linear model 不太适用于解 Classification 问题, 比如判断是否是肿瘤 (如果是判断概率是可以用的):</p>
<p><img src="/../img/machine-learning-linear-regression-for-tumor.png" srcset="/img/loading.gif" lazyload></p>
<p>(可以看到难以拟合其他数据, 毕竟分类问题不需要进行拟合, 其目的是找到一个 decision boundary)</p>
<p>一般用 logistic regression, 尽管名称中有 regression, 但其解决的是 classification 问题 (还有 decision tree, random forest 这些).</p>
<p>如果 target variable 只有两个元素, 比如:</p>
<ul>
<li>True, False</li>
<li>Yes, No</li>
<li>1, 0</li>
</ul>
<p>则称 “binary classification”. 而 <code>no</code>, <code>false</code>, <code>0</code> 称 nagative class, <code>yes</code>, <code>true</code>, <code>1</code> 称 positive class.</p>
<p>常见的 logistic function 为 sigmoid function, 可以将输入缩放到 <code>[0,1]</code>:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>g(z) &#x3D; \frac{1}{1 + e^{-z}}<br>\end{aligned}<br>}<br>$$</p>
<p><img src="/../img/machine-learning-sigmoid-function-example.png" srcset="/img/loading.gif" lazyload></p>
<p>Logistic regression 就是 sigmoid function 和 linear regression 的结合:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>f(\vec{x}) &#x3D; g(\vec{w} \cdot \vec{x} + b) &#x3D; \frac{1}{1 + e^{-(\vec{w} \cdot \vec{x} + b)}}<br>\end{aligned}<br>}<br>$$</p>
<p>其实就是把 linear regression 的结果映射到 <code>[0,1]</code> 中.</p>
<h1 id="28-Decision-boundary"><a href="#28-Decision-boundary" class="headerlink" title="28 Decision boundary"></a>28 Decision boundary</h1><p>Decision boundary 指, 设置一个 threshold 来处理 logistic regression 的概率输出, 超过 threshold 就得 <code>1</code>, 低于就得 <code>0</code>.</p>
<p>比如设置 <code>threshold=0.5</code>, 此时只要 <code>z&gt;0</code> 就输出 <code>1</code>, <code>z&lt;0</code> 就输出 <code>0</code>:</p>
<p><img src="/../img/machine-learning-decision-boundary-example.png" srcset="/img/loading.gif" lazyload></p>
<p>比如:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>g(z) &#x3D; g(w_1 x_1 + w_2 x_2 + b) \newline~ \newline<br>threshold &#x3D; 0.5<br>\end{aligned}<br>}<br>$$</p>
<p>此时有:<br>$$<br>\displaylines<br>{<br>\begin{aligned}<br>g(z) &#x3D; 0.5<br>\end{aligned}<br>}<br>$$</p>
<p>能计算出多组满足的 $w_1$, $w_2$, $b$, 这会对应一条分界线:</p>
<p><img src="/../img/machine-learning-decision-boundary-to-linear-line.png" srcset="/img/loading.gif" lazyload></p>
<p>如果 <code>z</code> 是多项式, 此时也可以得到 non-linear decision boundaries:</p>
<p><img src="/../img/machine-learning-non-linear-decision-boundaries.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="29-Cost-function-for-logistic-regression"><a href="#29-Cost-function-for-logistic-regression" class="headerlink" title="29 Cost function for logistic regression"></a>29 Cost function for logistic regression</h1><p>对于 logistic regression, 如果还选用 squared error cost, 则难以到达 global minimum, 其图像如下:</p>
<p><img src="/../img/machine-learning-cost-function-for-squared-error-cost-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>图像是 non-convex 的</li>
<li>有很多 local minimum</li>
</ul>
<p>选用其他 cost function, 已得到 convex 的图像, 便于 gradient descent 收敛.</p>
<p>这里先定义 loss function (一个样本点的误差衡量):</p>
<p><img src="/../img/machine-learning-loss-function-for-logistic-regression.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>当 $y^{(i)} &#x3D; 1$ 时, 需要保证 $f(\vec{x}$ 越接近 <code>1</code>, loss 越小</li>
<li>当 $y^{(i)} &#x3D; 0$ 时, 需要保证 $f(\vec{x}$ 越接近 <code>0</code>, loss 越小</li>
</ul>
<p>可以写在一个方程里:</p>
<p><img src="/../img/machine-learning-logistic-regression-loss-function.png" srcset="/img/loading.gif" lazyload></p>
<p>此时的 cost function 为:</p>
<p><img src="/../img/machine-learning-cost-function-for-logistic-regression.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="30-Overfitting"><a href="#30-Overfitting" class="headerlink" title="30 Overfitting"></a>30 Overfitting</h1><p>下面包含 regression:</p>
<ul>
<li>underfitting</li>
<li>just right</li>
<li>overfitting</li>
</ul>
<p>的例子:</p>
<p><img src="/../img/machine-learning-three-fit-example.png" srcset="/img/loading.gif" lazyload></p>
<p>Classification 的例子:</p>
<p><img src="/../img/machine-learning-three-fit-example-for-classification.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="31-解决-overfitting"><a href="#31-解决-overfitting" class="headerlink" title="31 解决 overfitting"></a>31 解决 overfitting</h1><p>一个方法是获取更多 training examples:</p>
<p><img src="/../img/machine-learning-collecting-more-training-examples.png" srcset="/img/loading.gif" lazyload></p>
<p>另一个方法是考虑使用更少的 features, 减少一些 insufficient data:</p>
<p><img src="/../img/machine-learning-using-less-feature-for-fix-overfitting.png" srcset="/img/loading.gif" lazyload></p>
<p>还有一个方法是 regularization, 其会保留所有特征, 但会减少特征对整个结果的影响 (比如减小参数):</p>
<p><img src="/../img/machine-learning-regularizatioin-example.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="34-带正则化的-cost-function"><a href="#34-带正则化的-cost-function" class="headerlink" title="34 带正则化的 cost function"></a>34 带正则化的 cost function</h1><p>添加迹象, 让参数乘以一个大数, 此时 cost function 想变小, 只能减小参数值:</p>
<p><img src="/../img/machine-learning-cost-function-with-regularization.png" srcset="/img/loading.gif" lazyload></p>
<p>同时惩罚所有参数的方法, 在原 cost function 的基础上添加 $\frac{\lambda}{2m}\sum_{j&#x3D;1}^n w_j^2$:</p>
<ul>
<li>$\lambda &gt; 0$ 是 regularization parameter</li>
</ul>
<p><img src="/../img/machine-learning-regularization-example-for-all-parameters.png" srcset="/img/loading.gif" lazyload></p>
<p>Regularization 在 gradient descent 中也会缓慢减小 $w$ 的值:</p>
<p><img src="/../img/machine-learning-regularization-effector-for-gradient-descent.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="36-Regularized-logistic-regression"><a href="#36-Regularized-logistic-regression" class="headerlink" title="36 Regularized logistic regression"></a>36 Regularized logistic regression</h1><p>同样在 cost function 中添加正则化项:</p>
<p><img src="/../img/machine-learning-regularized-logistic-regression.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="38-Deep-learning-intro"><a href="#38-Deep-learning-intro" class="headerlink" title="38 Deep learning intro"></a>38 Deep learning intro</h1><p>Deep Learning 是 Machine Learning 的一个子领域, 用多层神经网络自动学习数据特征表示. 不需要人工设计特征 (不同于 ML 需要特征工程), 模型直接从原始数据中学习抽象特征.</p>
<h1 id="39-神经元和大脑"><a href="#39-神经元和大脑" class="headerlink" title="39 神经元和大脑"></a>39 神经元和大脑</h1><p>Deep learning 中的一个 neuron 代表一个计算:</p>
<p><img src="/../img/deep-learning-neuron-example2.png" srcset="/img/loading.gif" lazyload></p>
<p>随着数据的增大, traditional AI 难以获取数据间的结构和关联, 而 neural network 包含大量参数, 能很好进行捕获:</p>
<p><img src="/../img/deep-learning-why-neural-network-important.png" srcset="/img/loading.gif" lazyload></p>
<p>之前学的 sigmoid 可以视为一个 activation, 意为: 激活一个信号给下一个 neuron.</p>
<h1 id="40-需求预测"><a href="#40-需求预测" class="headerlink" title="40 需求预测"></a>40 需求预测</h1><p>一个简单的 neuron network 示例:</p>
<p><img src="/../img/deep-learning-simple-neuron-network-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>只包含 input layer 和 outpur layer</li>
<li>Hidden layer 指 input layer 和 output layer 之间的所有 layer, 因为它们的输出我们不会直接看到</li>
</ul>
<p>多层示例:</p>
<p><img src="/../img/deep-learning-with-multiple-hidden-layers.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="41-图像识别"><a href="#41-图像识别" class="headerlink" title="41 图像识别"></a>41 图像识别</h1><p>比如 face recognition:</p>
<p><img src="/../img/deep-learning-face-recognition-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>输入是 1000x1000 的图像, 转成 1million 的输入 $\hat{x}$</li>
<li>输出人的身份 $y$</li>
</ul>
<p><img src="/../img/deep-learning-neuron-for-face-recognition.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="42-Neuron-network"><a href="#42-Neuron-network" class="headerlink" title="42 Neuron network"></a>42 Neuron network</h1><p>假设还是只有两层, 第一层的计算如下:</p>
<p><img src="/../img/deep-learning-neural-network-layer1-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>每一个神经元假设进行 sigmoid 计算, 有 <code>w</code> 和 <code>b</code> 两个参数</li>
</ul>
<p>第二层的计算如下:</p>
<p><img src="/../img/deep-learning-neural-network-layer2-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>因为只需要一个输出 (概率), 因此只需要一个神经元</li>
</ul>
<h1 id="43-More-complex-neuron-network"><a href="#43-More-complex-neuron-network" class="headerlink" title="43 More complex neuron network"></a>43 More complex neuron network</h1><p>一个 neuron network 的层数一般是 <code>hidden layer</code> + <code>output layer</code>, 因此下面示例可以看作是 4 layers:</p>
<p><img src="/../img/deep-learning-four-layers-neuron-network-example.png" srcset="/img/loading.gif" lazyload></p>
<p>Layer 3 的计算如下图所示:</p>
<p><img src="/../img/deep-learning-example-layer3-calculation.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="44-Inference-forward-propagation"><a href="#44-Inference-forward-propagation" class="headerlink" title="44 Inference: forward propagation"></a>44 Inference: forward propagation</h1><p>Neuron network 从输入计算到输出的过程称 forward propagation:</p>
<p><img src="/../img/deep-learning-forward-propagation-example.png" srcset="/img/loading.gif" lazyload></p>
<p>一个简单 neuron network 的 tensorflow 示例:</p>
<p><img src="/../img/deep-learning-simple-tensor-flow-example.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense<br>x = np.array([<span class="hljs-number">200.0</span>, <span class="hljs-number">17.0</span>])<br>layer_1 = Dense(units=<span class="hljs-number">3</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>a1 = layer_1(x)<br>layer_2 = Dense(units=<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br>a2 = layer_2(a1)<br></code></pre></td></tr></table></figure>

<h1 id="48-Forward-prop-in-single-layer"><a href="#48-Forward-prop-in-single-layer" class="headerlink" title="48 Forward prop in single layer"></a>48 Forward prop in single layer</h1><p>用 tensorflow 手动计算 forward propagation:</p>
<p><img src="/../img/deep-learning-calculate-forward-prop-by-hand-example.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">x = np.array([<span class="hljs-number">200</span>, <span class="hljs-number">17</span>])<br><br>w1_1 = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br>b1_1 = np.array([-<span class="hljs-number">1</span>])<br>z1_1 = np.dot(w1_1, x) + b1_1<br>a1_1 = sigmoid(z1_1)<br><br>w1_2 = np.array([-<span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br>b1_2 = np.array([<span class="hljs-number">1</span>])<br>z1_2 = np.dot(w1_2, x) + b1_2<br>a1_2 = sigmoid(z1_2)<br><br>w1_3 = np.array([<span class="hljs-number">5</span>, -<span class="hljs-number">6</span>])<br>b1_3 = np.array([<span class="hljs-number">2</span>])<br>z1_3 = np.dot(w1_3, x) + b1_3<br>a1_3 = sigmoid(z1_3)<br><br>a1 = np.array([a1_1, a1_2, a1_3])<br><br>w2_1 = np.array([-<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>])<br>b2_1 = np.array([<span class="hljs-number">3</span>])<br>z2_1 = np.dot(w2_1, a1) + b2_1<br>a2_1 = sigmoid(z2_1)<br></code></pre></td></tr></table></figure>

<p>可以定义一个 <code>dense</code> layer 来简化上述代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">dense</span>(<span class="hljs-params">a_in, W, b</span>):<br>    units = W.shape[<span class="hljs-number">1</span>]<br>    a_out = np.zeros(units)<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(units):<br>        w = W[:,j]<br>        z = np.dot(w,a_in) + b[j]<br>        a_out[j] = g(z)<br>    <span class="hljs-keyword">return</span> a_out<br></code></pre></td></tr></table></figure>

<p>另外可以定义一个 <code>sequential</code> 来串连几个 layers:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sequential</span>(<span class="hljs-params">x</span>):<br>    a1 = dense(x, W1, b1)<br>    a2 = dense(x, W2, b2)<br>    a3 = dense(x, W3, b3)<br>    a4 = dense(x, W4, b4)<br>    f_x = a4<br>    <span class="hljs-keyword">return</span> f_x<br></code></pre></td></tr></table></figure>

<h1 id="51-神经网络的高效实现方式"><a href="#51-神经网络的高效实现方式" class="headerlink" title="51 神经网络的高效实现方式"></a>51 神经网络的高效实现方式</h1><p>比如 <code>For loop</code> vs <code>vectorization</code> 的实现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># For loop example</span><br>x = np.array([<span class="hljs-number">200</span>, <span class="hljs-number">17</span>])<br>W = np.array([[<span class="hljs-number">1</span>, -<span class="hljs-number">3</span>, <span class="hljs-number">5</span>],<br>             [-<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, -<span class="hljs-number">6</span>]])<br>b = np.array([-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dense</span>(<span class="hljs-params">a_in, W, b</span>):<br>    units = W.shape[<span class="hljs-number">1</span>]<br>    a_out = np.zeros(units)<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(units):<br>        w = W[:,j]<br>        z = np.dot(w,a_in) + b[j]<br>        a_out[j] = g(z)<br>    <span class="hljs-keyword">return</span> a_out<br></code></pre></td></tr></table></figure>

<p>Vectorization example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">X = np.array([<span class="hljs-number">200</span>, <span class="hljs-number">17</span>])<br>W = np.array([[<span class="hljs-number">1</span>, -<span class="hljs-number">3</span>, <span class="hljs-number">5</span>],<br>              [-<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, -<span class="hljs-number">6</span>]])<br>B = np.array([[-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dense</span>(<span class="hljs-params">A_in, W, B</span>):<br>    Z = np.matmul(A_in, W) + B<br>    A_out = g(Z)<br>    <span class="hljs-keyword">return</span> A_out<br></code></pre></td></tr></table></figure>

<p>一个完整的 neuron network 的训练可以是:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> Sequential<br><span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense<br><span class="hljs-keyword">from</span> tensorflow.keras.losses <span class="hljs-keyword">import</span> BinaryCrossentropy<br><br>model = Sequential([<br>    Dense(units=<span class="hljs-number">25</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>),<br>    Dense(units=<span class="hljs-number">15</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>),<br>    Dense(units=<span class="hljs-number">1</span>, activation=<span class="hljs-string">&#x27;sigmoid&#x27;</span>),<br>])<br><br>model.<span class="hljs-built_in">compile</span>(loss=BinaryCrossentropy)<br>model.fit(X,Y,epochs=<span class="hljs-number">100</span>)<br></code></pre></td></tr></table></figure>

<h1 id="57-其他常见-activation-function"><a href="#57-其他常见-activation-function" class="headerlink" title="57 其他常见 activation function"></a>57 其他常见 activation function</h1><p><img src="/../img/deep-learning-other-activation-function-examples.png" srcset="/img/loading.gif" lazyload></p>
<p>activation function 的选取一般取决于 target variable 的类型:</p>
<ul>
<li>Binary classification 一般用 sigmoid (输出 $0~1$ 概率)</li>
<li>Regression 如股票涨跌 一般用 linear activation (输出 $0~1$)</li>
<li>Regression 如房价预测一般用 ReLU (输出正数)</li>
</ul>
<p>在 hidden layer 中一般都用 ReLU.</p>
<p><img src="/../img/deep-learning-why-ReLU-not-sigmoid.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>ReLU 的计算更快且 flat</li>
<li>sigmoid 一般用在输出层</li>
</ul>
<h1 id="59-Why-activation-function-is-needed"><a href="#59-Why-activation-function-is-needed" class="headerlink" title="59 Why activation function is needed"></a>59 Why activation function is needed</h1><p>如果不添加 activation function, neuron network 就和 linear model 没什么区别, 反之可以引入非线性关系, 更容易捕获一些结构.</p>
<p>一个 linear example:</p>
<p><img src="/../img/deep-learning-neuron-network-only-linear-model.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="60-Multiclass"><a href="#60-Multiclass" class="headerlink" title="60 Multiclass"></a>60 Multiclass</h1><p>输出为一个数量大于 2 的集合, 而非 binary classification, 比如:</p>
<p><img src="/../img/deep-learning-multiclass-problem-example.png" srcset="/img/loading.gif" lazyload></p>
<p>此时, 要输出每个类别的概率, 需要用到 softmax activation.</p>
<h1 id="61-Softmax"><a href="#61-Softmax" class="headerlink" title="61 Softmax"></a>61 Softmax</h1><p>Softmax regression algorithm 是逻辑回归的泛化, 用于 multiclass 的场景. 所有可能的类别输出概率相加为 1.</p>
<p><img src="/../img/deep-learning-how-softmax-calculate.png" srcset="/img/loading.gif" lazyload></p>
<p>Loss function:</p>
<p><img src="/../img/deep-learning-loss-function-for-softmax.png" srcset="/img/loading.gif" lazyload></p>
<p>一个在 neuron network 中使用 softmax 的例子:</p>
<p><img src="/../img/deep-learning-softmax-in-neuron-network-example.png" srcset="/img/loading.gif" lazyload></p>
<p>一个 Tensorflow 的代码示例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> tf.keras <span class="hljs-keyword">import</span> Sequential<br><span class="hljs-keyword">from</span> tf.keras.layers <span class="hljs-keyword">import</span> Dense<br><span class="hljs-keyword">from</span> tf.keras.losses <span class="hljs-keyword">import</span> SparseCategoricalCrossentropy<br><br>model = Sequential([<br>    Dense(units=<span class="hljs-number">25</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>),<br>    Dense(units=<span class="hljs-number">15</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>),<br>    Dense(units=<span class="hljs-number">10</span>, activation=<span class="hljs-string">&quot;softmax&quot;</span>),<br>])<br><br>model.<span class="hljs-built_in">compile</span>(loss=SparseCategoricalCrossentropy())<br>model.fit(X,Y,epochs=<span class="hljs-number">100</span>)<br></code></pre></td></tr></table></figure>

<p>或写为:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> tf.keras <span class="hljs-keyword">import</span> Sequential<br><span class="hljs-keyword">from</span> tf.keras.layers <span class="hljs-keyword">import</span> Dense<br><span class="hljs-keyword">from</span> tf.keras.losses <span class="hljs-keyword">import</span> SparseCategoricalCrossentropy<br><br>model = Sequential([<br>    Dense(units=<span class="hljs-number">25</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>),<br>    Dense(units=<span class="hljs-number">15</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>),<br>    Dense(units=<span class="hljs-number">10</span>, activation=<span class="hljs-string">&quot;linear&quot;</span>),<br>])<br><br>model.<span class="hljs-built_in">compile</span>(loss=SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>))<br>model.fit(X,Y,epochs=<span class="hljs-number">100</span>)<br><br>logits = model(X)<br>f_x = tf.nn.softmax(logits)<br></code></pre></td></tr></table></figure>

<ul>
<li>添加 <code>fromo_logits=True</code> 参数能减少 round off error</li>
</ul>
<h1 id="64-Multilabel-problem"><a href="#64-Multilabel-problem" class="headerlink" title="64 Multilabel problem"></a>64 Multilabel problem</h1><p>等价于多个 binary classification 问题:</p>
<p><img src="/../img/deep-learning-how-to-deal-with-multilabel-problem.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="Advanced-Optimization"><a href="#Advanced-Optimization" class="headerlink" title="Advanced Optimization"></a>Advanced Optimization</h1><p>Gradient descent 由于 alpha 的值很小, 缓慢向 minimum 靠近, 此时可以考虑增大 alpha 的值加快收敛. 且如果初始 alpha 很大, 导致震荡, 此时考虑减小 alpha 的值. “Adam” algorithm 所用的就是这个思想, 如果其发现 alpha 很小且一直向一个方向收敛, 则会增大 alpha 的值, 反之:</p>
<p><img src="/../img/deep-learning-adam-algorithm-example.png" srcset="/img/loading.gif" lazyload></p>
<p>Adam (Adaptive Moment estimation) algorithm 会自动调整 learning rate (alpha), 还会为每一组 loss function 设置一个 learning rate 值.</p>
<p>Tensorflow 中的使用示例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tf.keras.optimizers <span class="hljs-keyword">import</span> Adam<br><br>model.<span class="hljs-built_in">compile</span>(optimizer=Adam(learning_rate=<span class="hljs-number">1e-3</span>))<br></code></pre></td></tr></table></figure>

<h1 id="66-Additional-layer-type"><a href="#66-Additional-layer-type" class="headerlink" title="66 Additional layer type"></a>66 Additional layer type</h1><p>Dense layer (全连接层) 中, 每一个 neuron 代表一个函数运算, 输入是前一层的输出:</p>
<p><img src="/../img/deep-learning-detail-of-dense-layer-example.png" srcset="/img/loading.gif" lazyload></p>
<p>Convolutional layer (卷积层), 每一个 neuron 的输入是前一层的部分输出:</p>
<p><img src="/../img/deep-learning-convolution-neuron-network-intro.png" srcset="/img/loading.gif" lazyload></p>
<p>由 convolutional layer 组成的 network 一般称 convolutional neural network:</p>
<p><img src="/../img/deep-learning-convolutional-network-example.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="68-Computation-graph"><a href="#68-Computation-graph" class="headerlink" title="68 Computation graph"></a>68 Computation graph</h1><p>Computation graph 是一种用于描述数学运算的有向图, 其中:</p>
<ul>
<li>Node (节点), 表示变量 (输入, 输出, 中间结果等) 或者操作 (如加法, 矩阵乘法)</li>
<li>Edge (边), 表示数据依赖关系 (操作的输入或输出流向)</li>
</ul>
<p>在 Tensorflow, PyTorch 等框架中, 用于实现 Autograd (自动微分).</p>
<p><img src="/../img/deep-learning-computation-graph-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>借助链式法则, 从外向里计算偏导数, 正好是反向传播的方向</li>
</ul>
<h1 id="70-How-to-improve"><a href="#70-How-to-improve" class="headerlink" title="70 How to improve"></a>70 How to improve</h1><p>一些常见思路:</p>
<ul>
<li>Get more training examples, 解决 high variance</li>
<li>Try smaller sets of features, 解决 high variance</li>
<li>Try getting additional features, 解决 high bias</li>
<li>Try adding polynomial features, 解决 high bias</li>
<li>Try decreasing $\lambda$, 解决 high bias</li>
<li>Try increasing $\lambda$, 解决 high variance</li>
</ul>
<h1 id="71-Evaluating-a-model"><a href="#71-Evaluating-a-model" class="headerlink" title="71 Evaluating a model"></a>71 Evaluating a model</h1><p>一个方法是, 将 dataset 拆分为 training set 和 test set, 在用 training set 训练好的模型上, 用 test set 来验证:</p>
<p><img src="/../img/deep-learning-evaluate-with-test-set.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="73-Bias-and-variance"><a href="#73-Bias-and-variance" class="headerlink" title="73 Bias and variance"></a>73 Bias and variance</h1><p>可以通过观察 training 和 cross validation 的结果来判断 bias 和 variance 情况.</p>
<ul>
<li>$J_{train}$ 高表示 bias 高</li>
<li>$J_{crossvalidation}$ 高表示 variance 高</li>
</ul>
<p><img src="/../img/deep-learning-from-curve-get-bias-variance-info.png" srcset="/img/loading.gif" lazyload></p>
<p>Bias 和 variance 和 polynomial 次数有关:</p>
<ul>
<li>次数增大, bias 越小, $J_{train}$ 越小</li>
<li>次数增大, variance 先减小后增大, $J_{cv}$ 表现越大</li>
</ul>
<p><img src="/../img/deep-learning-bias-and-variance-with-polynomial.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>找到中间点比较重要</li>
</ul>
<p>当 polynomial 较大时, 可以借助 regularization 来降低 variance. 可以通过调整 $\lambda$ 的值来选择:</p>
<p><img src="/../img/deep-learning-try-to-find-a-good-lambda.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="75-Establishing-a-baseline-level-of-performance"><a href="#75-Establishing-a-baseline-level-of-performance" class="headerlink" title="75 Establishing a baseline level of performance"></a>75 Establishing a baseline level of performance</h1><p>Baseline 是指, 在正式优化或改进一个系统之前, 需要先确定的一个初始的, 可比较的性能标准. 之后可以根据该性能表现作为起点来改进.</p>
<p>一般:</p>
<ul>
<li>Baseline performance 与 training error 比较得到一个差值</li>
<li>Training error 与 cross validation error 比较得到一个差值</li>
</ul>
<p><img src="/../img/deep-learning-how-to-use-baseline-model.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="76-Learning-curves"><a href="#76-Learning-curves" class="headerlink" title="76 Learning curves"></a>76 Learning curves</h1><p>Learning curves 指模型性能随训练数据量或训练迭代次数变化的曲线, 比如:</p>
<p><img src="/../img/deep-learning-learning-curves-example.png" srcset="/img/loading.gif" lazyload></p>
<p>对于一个 high bias 的模型而言, learning curve 如下:</p>
<p><img src="/../img/deep-learning-learning-curve-for-high-bias-model.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>在增加数据量之前, 判断自己的模型是否有 high bias</li>
</ul>
<p>对于一个 high variance 的模型而言, learning curve 如下:</p>
<p><img src="/../img/deep-learning-learning-rate-for-high-variance-model.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="78-Bias-variance-tradeoff"><a href="#78-Bias-variance-tradeoff" class="headerlink" title="78 Bias variance tradeoff"></a>78 Bias variance tradeoff</h1><p>在机器学习中, 一般来说有:</p>
<ul>
<li>simple model 对应 high bias</li>
<li>complex model 对应 high variance</li>
</ul>
<p><img src="/../img/deep-learning-bias-variance-tradeoff-example.png" srcset="/img/loading.gif" lazyload></p>
<p>Large neuron network 通常是 low bias 的, 因此, 在开发中:</p>
<ul>
<li>首先判断在 train set 上的表现, 如果 loss 很大, 说明 bias 较大 (欠拟合), 考虑增大网络</li>
<li>之后判断在 cross validation set 上的表现, 如果 loss 很大, 说明 variance 较大 (过拟合), 考虑增加输入数据</li>
</ul>
<p><img src="/../img/deep-learning-neural-network-and-bias-variance-example.png" srcset="/img/loading.gif" lazyload></p>
<p>Large neuron network 配合适当 regularization 可以减少过拟合的风险.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">layer_1 = Dense(units=<span class="hljs-number">25</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>, kernel_regularizer=L2(<span class="hljs-number">0.01</span>))<br></code></pre></td></tr></table></figure>
<ul>
<li><code>0.01</code> 是初始化的 $\lambda$ 值</li>
</ul>
<h1 id="79-Iterative-loop-of-ML-development"><a href="#79-Iterative-loop-of-ML-development" class="headerlink" title="79 Iterative loop of ML development"></a>79 Iterative loop of ML development</h1><p>三个步骤循环:</p>
<ul>
<li>选择 model, data, 以及 hyper parameter 等</li>
<li>训练模型</li>
<li>通过 bias, variance, error analysis 来诊断</li>
</ul>
<p><img src="/../img/deep-learning-iterative-loop-of-ml-development.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="81-Adding-data"><a href="#81-Adding-data" class="headerlink" title="81 Adding data"></a>81 Adding data</h1><p>对于某些领域如图像处理, 可以考虑 data augmentation (数据增强) 来获取新 data: 修改已有的 training example 来得到新的 training example.</p>
<p><img src="/../img/deep-learning-data-augmentation-example1.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/../img/deep-learning-data-augmentation-example2.png" srcset="/img/loading.gif" lazyload></p>
<p>在 speech 领域, 可以考虑引入噪音:</p>
<p><img src="/../img/deep-learning-data-augmentation-of-speech-example.png" srcset="/img/loading.gif" lazyload></p>
<p>注意不添加纯粹的无意义的 noise.</p>
<p>另一个技巧是 data synthesis, 手动创建全新的数据.</p>
<h1 id="82-Transfer-learning"><a href="#82-Transfer-learning" class="headerlink" title="82 Transfer learning"></a>82 Transfer learning</h1><p>Transfer learning 指, 使用其他模型的参数作为起始参数 (如果输出维度不同, 则只使用 hidden layer 的参数).</p>
<p>比如你想完成的任务只有很少的数据 (比如 1k), 可以先在其他大数据集 (1million) 上训练神经网络, 然后使用其参数, 之后再用小数据集训练:</p>
<ul>
<li>可以只调整 output layer 的参数</li>
<li>或者全部参数</li>
</ul>
<p><img src="/../img/deep-learning-transfer-learning-example.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="83-Full-cycle-of-a-machine-learning-project"><a href="#83-Full-cycle-of-a-machine-learning-project" class="headerlink" title="83 Full cycle of a machine learning project"></a>83 Full cycle of a machine learning project</h1><p><img src="/../img/deep-learning-full-cycle-of-a-machine-learning-project.png" srcset="/img/loading.gif" lazyload></p>
<p>Deployment 的一些细节:</p>
<p><img src="/../img/deep-learning-deployment-example.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="85-Precision-and-Recall"><a href="#85-Precision-and-Recall" class="headerlink" title="85 Precision and Recall"></a>85 Precision and Recall</h1><p>Confusion Matrix, 混淆矩阵, 是用于评估分类模型性能的工具, 它将预测结果与真实标签进行对比, 在二分类中, 结构如下:</p>
<p><img src="/../img/deep-learning-confusion-matrix-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>True 意思是模型预测正确</li>
<li>False 意思是模型预测错误</li>
<li>Positive 意思是正类样本</li>
<li>Negative 意思是负类样本</li>
</ul>
<p>Precision 指, 在所有 “预测为正例” 的样本中, 有多少是真正的 “正例”. 用于衡量预测准确性 (避免误报). 高精确率意味着: 当模型预测为正例时, 结果可信度高.</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>Precision &amp;&#x3D; \frac{TP}{TP + FP} \newline~ \newline<br>&amp;&#x3D; \frac{true positive}{total predicted positive}<br>\end{aligned}<br>}<br>$$</p>
<ul>
<li>TP, 指 True Positive, 真正例, 模型正确预测为正类的样本 (实际是猫, 预测也是猫)</li>
<li>FP, 指 False Positive, 假正例, 模型错误预测为正类的样本 (实际是狗, 但预测为猫)</li>
<li>FN, 指 False Negative, 假负例, 模型错误预测为负类的样本 (实际是猫, 但预测为狗)</li>
<li>TN, True Negative, 真负例, 模型正确预测为负类的样本 (实际是狗, 预测也是狗)</li>
</ul>
<p>Recall, 召回率 (查全率), 指在所有 “实际为正例” 的样本中, 模型正确预测了多少. 用于衡量覆盖能力 (避免捡漏), 高召回率意味着: 能捕捉到大部分真实正例.</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>Recall &amp;&#x3D; \frac{TP}{TP + FN} \newline~ \newline<br>&amp;&#x3D; \frac{true positive}{total actual positive}<br>\end{aligned}<br>}<br>$$</p>
<p><img src="/../img/deep-learning-precision-and-recall-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>Actual class 指样本实际是什么类别</li>
<li>Predicted class 指模型预测的样本的类别</li>
</ul>
<p>Precision 和 recall 值都很高说明模型比较好.</p>
<h1 id="86-Trading-off-precision-and-recall"><a href="#86-Trading-off-precision-and-recall" class="headerlink" title="86 Trading off precision and recall"></a>86 Trading off precision and recall</h1><p>假设我们用 logistic regression 处理二分类问题, 输出一个概率值, 然后与 threshold 比较, 从而判断其类别, 那么有:</p>
<ul>
<li>Threshold 越大, precision 越大, recall 越小</li>
<li>Threshold 越小, precision 越小, recall 越大</li>
</ul>
<p><img src="/../img/deep-learning-threshold-with-precision-and-recall.png" srcset="/img/loading.gif" lazyload></p>
<p>可以用 F1 Score 来结合 precision 和 recall 来综合得出一个分数, 判断算法的好坏:</p>
<p>$$<br>\displaylines<br>{<br>\begin{aligned}<br>F1 &#x3D; \frac{1}{ \frac{1}{2} ( \frac{1}{P} + \frac{1}{R})} &#x3D; 2 \frac{RP}{P + R}<br>\end{aligned}<br>}<br>$$<br>(数学上也称 Harmonic mean, 其强调较小的数字的作用)</p>
<p><img src="/../img/deep-learning-f1-score-example.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="87-Decision-tree-model"><a href="#87-Decision-tree-model" class="headerlink" title="87 Decision tree model"></a>87 Decision tree model</h1><p>Decision tree 是一个树形结构的监督学习算法, 其:</p>
<ul>
<li>节点 (包括根节点), 表示用于分类的规则</li>
<li>枝干, 表示分类后的小数据集</li>
<li>叶子节点, 表示预测的类型</li>
</ul>
<p>根节点一般用最 powerful 的 feature 来进行分类.</p>
<p>在构建 decision tree 时, 需要判断:</p>
<ul>
<li>选择哪些 features</li>
<li>什么时候停止 split (树的 depth)</li>
</ul>
<h1 id="88-Measuring-purity"><a href="#88-Measuring-purity" class="headerlink" title="88 Measuring purity"></a>88 Measuring purity</h1><p>如果样本全是单一类别, 就说 purity 很高.</p>
<p>Purity 的判断可以基于熵 (H, entropy).</p>
<p>在二分类中, entropy 可以这样计算:</p>
<p><img src="/../img/deep-learning-entropy-example1.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="89-Information-gain"><a href="#89-Information-gain" class="headerlink" title="89 Information gain"></a>89 Information gain</h1><p>熵减少的程度称 Information gain (信息增益). 可以用来判断那个特征更高.</p>
<p>用原始的熵值, 减去 split 之后的熵值, 就是 information gain, 越大, 说明 feature 越重要:</p>
<p><img src="/../img/deep-learning-information-gain-example.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/../img/deep-learning-information-gain-example2.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="91-Decision-Tree-learning-过程"><a href="#91-Decision-Tree-learning-过程" class="headerlink" title="91 Decision Tree learning 过程"></a>91 Decision Tree learning 过程</h1><ol>
<li>计算所有 possible features 的 information gain, 选择最大的作为 root node</li>
<li>用 selected feature 拆分 dataset, 得到 left and right branches</li>
<li>再次计算 information gain 并选择下一个 node 持续 spliting, 直到 stopping criteria, 如:<br> a. 一个 node 100% 是一个类别<br> b. Tree 达到 maximum depth<br> c. Information gain 少于 threshold<br> d. 进入 node 的样本数少于 threshold</li>
</ol>
<p>一个递归的过程.</p>
<p>可以用 one-hot encoding 处理 categorical variable, 用不等式处理 continuous variable.</p>
<h1 id="94-Regression-with-decision-tree"><a href="#94-Regression-with-decision-tree" class="headerlink" title="94 Regression with decision tree"></a>94 Regression with decision tree</h1><p>预测值等于 leaf node 中的平均值:</p>
<p><img src="/../img/deep-learning-regression-with-decision-tree-example.png" srcset="/img/loading.gif" lazyload></p>
<p>此时判断 feature 好坏的标准是计算 leaf node 中的方差 (variance):</p>
<p><img src="/../img/" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>原始方差与拆分后的加权方差之差越大越好</li>
</ul>
<p><img src="/../img/deep-learning-select-feature-for-regression-dt.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="95-Tree-ensembles"><a href="#95-Tree-ensembles" class="headerlink" title="95 Tree ensembles"></a>95 Tree ensembles</h1><p>用多个 decision tree 来构建模型的方法称 tree ensembles.</p>
<p>比如:</p>
<p><img src="/../img/deep-learning-tree-ensemble-example.png" srcset="/img/loading.gif" lazyload></p>
<p>最终结果由投票机制决定.</p>
<h1 id="97-Bagged-decision-trees"><a href="#97-Bagged-decision-trees" class="headerlink" title="97 Bagged decision trees"></a>97 Bagged decision trees</h1><p>一个 Bagged decision tree 示例, 每次通过 sampling with replacement 来选择样本训练 decision tree:</p>
<p><img src="/../img/deep-learning-random-forest-example.png" srcset="/img/loading.gif" lazyload></p>
<p>对该思路做点改进就是 random forest: 设定一个 k 值 ($k &lt; n$, n 是 feature 数量), 随机选择 k 个 features 来构建 decision tree, 构建多个.</p>
<p><img src="/../img/deep-learning-random-forest-example2.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="98-Boosted-decision-trees"><a href="#98-Boosted-decision-trees" class="headerlink" title="98 Boosted decision trees"></a>98 Boosted decision trees</h1><p>Boosted decision trees 的核心思想时, 每次不从全部样本中随机采样, 而是从上一次训练的树中, missclassified 的样本中采样 (纠正前一个模型的错误).</p>
<h1 id="99-When-to-use-decision-tree"><a href="#99-When-to-use-decision-tree" class="headerlink" title="99 When to use decision tree"></a>99 When to use decision tree</h1><p>Decision trees 和 tree ensembles 用于结构化的数据, 且解释性强, 训练速度快. 但对于 <code>images</code>, <code>audio</code>, <code>text</code> 等非结构化数据, 不建议使用. Tree ensemble 一般选择 xgboost 实现.</p>
<p>Neural network 对所有类别的数据都适用, 但训练速度慢, 也适用于 transfer learning.</p>
<h1 id="101-Unsupervised-learning-recommender-systems-以及-reinforcement-learning-介绍"><a href="#101-Unsupervised-learning-recommender-systems-以及-reinforcement-learning-介绍" class="headerlink" title="101 Unsupervised learning, recommender systems, 以及 reinforcement learning 介绍"></a>101 Unsupervised learning, recommender systems, 以及 reinforcement learning 介绍</h1><p>在 unsupervised learning 方向, 会涉及:</p>
<ul>
<li>Clustering</li>
<li>Anomaly detection</li>
</ul>
<p>Recommender systems, 一般用于广告推荐等.</p>
<p>Reinforcement learning 用于 video games, robots 等.</p>
<h1 id="102-Clustering"><a href="#102-Clustering" class="headerlink" title="102 Clustering"></a>102 Clustering</h1><p>Clustering algorithm 会自动找到一组数据点中的关联或相似性. 判断是否能 grouped into clusters.</p>
<p>在 supervised learning 中, 训练数据如:</p>
<p><img src="/../img/supervised-learning-training-set-example.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>标记了 input variable 和 target variable</li>
</ul>
<p>而在 unsupervised learning 中, 训练数据如:</p>
<p><img src="/../img/unsupervised-learning-data-set-example.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="103-K-means-intuition"><a href="#103-K-means-intuition" class="headerlink" title="103 K-means intuition"></a>103 K-means intuition</h1><p>K-means 是一个经典的无监督学习 clustering algorithm, 其核心思想是让同一 group 内的数据点尽可能相似, 不同 group 的数据点尽可能不同.</p>
<p>假设要找到两个 clusters, 先随机初始化 clusters 的中心 (称 cluster centroids):</p>
<p><img src="/../img/k-means-init-centers.png" srcset="/img/loading.gif" lazyload></p>
<ol>
<li>之后, 每个样本点判断离哪个 centroid 更近, 从而判断其属于那一个 group:</li>
</ol>
<p><img src="/../img/k-means-calculate-which-group-belong-to.png" srcset="/img/loading.gif" lazyload></p>
<ol start="2">
<li>然后计算 group 内数据点的平均值, 形成新的 centroid (centroid 移动到新的位置):</li>
</ol>
<p><img src="/../img/k-means-move-centroid-example.png" srcset="/img/loading.gif" lazyload></p>
<ol start="3">
<li>再次判断每个数据点属于哪一个 centroid (重新 group):</li>
</ol>
<p><img src="/../img/k-means-calculate-new-group.png" srcset="/img/loading.gif" lazyload></p>
<p>…</p>
<p>重复上述过程, 直到 centroid 不再移动, 此时 k-means 算法收敛:</p>
<p><img src="/../img/k-means-reach-to-end-example.png" srcset="/img/loading.gif" lazyload></p>
<p>K-means 的两个关键步骤是:</p>
<ol>
<li>根据与 cluster centroid 的距离分配数据点</li>
<li>将 cluster centroid 移动到 group 内数据点的平均值</li>
</ol>
<p>详细定义:</p>
<p><img src="/../img/k-means-algorithm-detail-definition.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>K 表示有 K 个 cluster centroids</li>
<li>Means 表示 centroid 的移动依靠求 means</li>
<li>如果一个 cluster 内没有分配到任何数据点, 则消除</li>
</ul>
<p>K-means 的 cost function 为 (可以用来判断是否收敛):</p>
<p><img src="/../img/k-means-cost-function-example.png" srcset="/img/loading.gif" lazyload></p>
<p>(该函数也称 distortion function)</p>
<ul>
<li>目标是减小数据点到 cluster centroid 的距离</li>
<li>该目标也说明了要将数据点分配到最近的 cluster centroid, 以及 cluster centroid 朝 mean value 移动更好</li>
<li>当 cost function 收敛时也能说明 k-means 结束</li>
</ul>
<h1 id="106-Initializing-K-means"><a href="#106-Initializing-K-means" class="headerlink" title="106 Initializing K-means"></a>106 Initializing K-means</h1><p>在初始化 K-means 时, 选择的 cluster 数量 K 需要小于训练样本 m.</p>
<p>一个初始化 cluster centroid 的方法是:</p>
<ul>
<li>随机选取 K training examples 作为 centroid</li>
</ul>
<p><img src="/../img/k-means-random-initialization-example.png" srcset="/img/loading.gif" lazyload></p>
<p>随机初始化的位置不一定好:</p>
<p><img src="/../img/k-means-initialization-with-bad-choice.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>可以通过多次运行 k-means 然后取最优的 (选 cost function 的结果更小的)</li>
</ul>
<p>多次初始化并计算到收敛的示例:</p>
<p><img src="/../img/k-means-random-initialization-multiple-times-example.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="107-Choosing-the-number-of-clusters"><a href="#107-Choosing-the-number-of-clusters" class="headerlink" title="107 Choosing the number of clusters"></a>107 Choosing the number of clusters</h1><p>对于大部分问题而言 K-means 算法要选取的 K 的大小是不确定的. 比如下面示例, 可以是 <code>K=2</code>, 也可以是 <code>K=4</code>:</p>
<p><img src="/../img/k-means-what-is-the-right-value-of-k.png" srcset="/img/loading.gif" lazyload></p>
<p>一个尝试 K 值的方法是 Elbow, 其绘制各 K 值的 cost function, 对比好坏, 但这不是一个好方法, 因为 K 越大, 通常 cost function 值越低:</p>
<p><img src="/../img/k-means-using-elbow-method-to-find-k.png" srcset="/img/loading.gif" lazyload></p>
<p>另一个方法是根据自己的目标来设置 K, 比如收集了一些人的 weight, height 数据, 判断 SML (small, middle, large) 哪个 size 的衣服合适, 此时就设置 <code>K=3</code>:</p>
<p><img src="/../img/k-means-find-k-by-purpose.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="108-Anomaly-Detection"><a href="#108-Anomaly-Detection" class="headerlink" title="108 Anomaly Detection"></a>108 Anomaly Detection</h1><p>Anomaly detection algorithm 会查看一个未标记的正常事件数据集, 并从中学习如何检测或标记异常事件.</p>
<p>比如飞机质量检测问题, 有原始 dataset ${x^{(1)}, x^{(2)},…, x^{(m)}}$, 在飞机结束航行后, 出现了新 feature $x_{test}$, 导致原有 feature 出现了一些变化:</p>
<p><img src="/../img/anomaly-detection-intro-example.png" srcset="/img/loading.gif" lazyload></p>
<p>一种常见的 anomaly detection 的方法是 density estimation: 从 dataset 中学习到, 哪些区域的值是属于 dataset 的 (是正常的). 输入 $x_{test}$ 得到一个概率输出, 如果低于某个阈值 $\epsilon$ 则认定为 anomaly:</p>
<p><img src="/../img/anomaly-detection-density-estimation-example.png" srcset="/img/loading.gif" lazyload></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>机器学习-吴恩达-Notes</div>
      <div>http://example.com/2025/08/11/机器学习-吴恩达-Notes/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Jie</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年8月11日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/08/12/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" title="对比学习">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">对比学习</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/08/11/Transformer-%E6%9E%B6%E6%9E%84/" title="Transformer-架构">
                        <span class="hidden-mobile">Transformer-架构</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'zKurisu/comments-utterances');
      s.setAttribute('issue-term', 'pathname');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Jie</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Orkarin</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
